#!/usr/bin/env python3
"""
è°ƒè¯•åŒè¯­æ‚£è€…æŒ‡å—è§£æé—®é¢˜
"""

import sys
import os
from pathlib import Path
from bs4 import BeautifulSoup
import requests

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def debug_patient_guidelines_page():
    """è°ƒè¯•æ‚£è€…æŒ‡å—é¡µé¢ç»“æ„"""
    print("ğŸ” è°ƒè¯•åŒè¯­æ‚£è€…æŒ‡å—é¡µé¢ç»“æ„...")
    print("=" * 60)

    try:
        # ä½¿ç”¨Cookieè®¿é—®æ‚£è€…æŒ‡å—é¡µé¢
        with open('extracted_cookies.txt', 'r', encoding='utf-8') as f:
            cookie_string = f.read().strip()

        # è§£æCookie
        cookies = {}
        for item in cookie_string.split(';'):
            if '=' in item:
                key, value = item.strip().split('=', 1)
                cookies[key] = value

        # åˆ›å»ºsession
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        session.cookies.update(cookies)

        # è®¿é—®æ‚£è€…æŒ‡å—ä¸»é¡µ
        url = "https://www.nccn.org/patientresources/patient-resources/guidelines-for-patients"
        print(f"ğŸŒ è®¿é—®: {url}")

        response = session.get(url)
        print(f"ğŸ“¡ çŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“„ é¡µé¢å¤§å°: {len(response.text):,} å­—ç¬¦")

        if response.status_code != 200:
            print("âŒ è®¿é—®å¤±è´¥")
            return

        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')

        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        all_links = soup.find_all('a', href=True)
        print(f"ğŸ”— æ€»é“¾æ¥æ•°: {len(all_links)}")

        # æŸ¥æ‰¾è¯¦æƒ…é¡µé“¾æ¥ (æ ¹æ®ç”¨æˆ·æä¾›çš„ç»“æ„)
        detail_links = []
        patient_links = []

        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)

            # æŸ¥æ‰¾è¯¦æƒ…é¡µé“¾æ¥
            if '/guidelines-for-patients-details?patientGuidelineId=' in href:
                detail_links.append({
                    'href': href,
                    'text': text,
                    'full_url': 'https://www.nccn.org' + href if href.startswith('/') else href
                })

            # æŸ¥æ‰¾PDFé“¾æ¥
            if '/patients/guidelines/content/PDF/' in href:
                patient_links.append({
                    'href': href,
                    'text': text,
                    'is_chinese': '-zh' in href.lower(),
                    'full_url': 'https://www.nccn.org' + href if href.startswith('/') else href
                })

        print(f"\nğŸ“‹ è¯¦æƒ…é¡µé“¾æ¥æ•°é‡: {len(detail_links)}")
        for i, link in enumerate(detail_links[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   {i+1}. {link['text']} -> {link['full_url']}")

        print(f"\nğŸ“‹ ç›´æ¥PDFé“¾æ¥æ•°é‡: {len(patient_links)}")
        chinese_count = 0
        english_count = 0
        for link in patient_links[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            version = "Chinese" if link['is_chinese'] else "English"
            if link['is_chinese']:
                chinese_count += 1
            else:
                english_count += 1
            print(f"   ğŸ“„ {link['text']} ({version}) -> {link['full_url'][:60]}...")

        print(f"\nğŸ“Š è¯­è¨€åˆ†å¸ƒ:")
        print(f"   ä¸­æ–‡ç‰ˆæœ¬: {chinese_count}")
        print(f"   è‹±æ–‡ç‰ˆæœ¬: {english_count}")

        # å¦‚æœæ‰¾åˆ°äº†è¯¦æƒ…é¡µé“¾æ¥ï¼Œæµ‹è¯•è®¿é—®ç¬¬ä¸€ä¸ª
        if detail_links:
            print(f"\nğŸ§ª æµ‹è¯•è®¿é—®ç¬¬ä¸€ä¸ªè¯¦æƒ…é¡µ...")
            test_detail_url = detail_links[0]['full_url']
            print(f"ğŸŒ è®¿é—®è¯¦æƒ…é¡µ: {test_detail_url}")

            detail_response = session.get(test_detail_url)
            print(f"ğŸ“¡ çŠ¶æ€ç : {detail_response.status_code}")

            if detail_response.status_code == 200:
                detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                detail_links_page = detail_soup.find_all('a', href=True)

                pdf_on_detail_page = []
                for link in detail_links_page:
                    href = link.get('href', '')
                    if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                        pdf_on_detail_page.append({
                            'href': href,
                            'text': link.get_text(strip=True),
                            'is_chinese': '-zh' in href.lower()
                        })

                print(f"ğŸ“‹ è¯¦æƒ…é¡µPDFé“¾æ¥æ•°é‡: {len(pdf_on_detail_page)}")
                for link in pdf_on_detail_page:
                    version = "Chinese" if link['is_chinese'] else "English"
                    full_url = 'https://www.nccn.org' + link['href'] if link['href'].startswith('/') else link['href']
                    print(f"   ğŸ“„ {link['text']} ({version}) -> {full_url}")

    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_patient_guidelines_page()