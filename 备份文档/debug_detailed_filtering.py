#!/usr/bin/env python3
"""
è¯¦ç»†è°ƒè¯•é€‰é¡¹1çš„è¯­è¨€è¿‡æ»¤è¿‡ç¨‹
æŸ¥çœ‹PDFè§£æå’Œè¿‡æ»¤çš„å…·ä½“æ­¥éª¤
"""

import sys
import os
import time
import random
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def debug_detailed_filtering():
    """è¯¦ç»†è°ƒè¯•è¯­è¨€è¿‡æ»¤è¿‡ç¨‹"""
    print("ğŸ” è¯¦ç»†è°ƒè¯•é€‰é¡¹1çš„è¯­è¨€è¿‡æ»¤è¿‡ç¨‹...")
    print("=" * 60)

    try:
        # è¯»å–é…ç½®æ–‡ä»¶
        config_file = 'config.json'
        with open(config_file, 'r', encoding='utf-8') as f:
            config_data = json.load(f)

        # åˆå§‹åŒ–ä¸‹è½½å™¨
        from download_NCCN_Guide_v2_menu import NCCNDownloaderV2, ThemeConfig

        downloader = NCCNDownloaderV2(config_data)

        # åˆ›å»ºé€‰é¡¹1çš„é…ç½®
        theme = ThemeConfig(
            name='cancer_treatment',
            display_name='ç™Œç—‡æ²»ç–—æŒ‡å—è‹±æ–‡ç‰ˆ (Treatment by Cancer Type - English Only)',
            url='https://www.nccn.org/guidelines/category_1',
            category='category_1',
            directory='01_Cancer_Treatment',
            description='æŒ‰ç™Œç—‡ç±»å‹åˆ†ç±»çš„æ²»ç–—æŒ‡å—ï¼ˆè‹±æ–‡ç‰ˆï¼‰',
            has_language_filter=True
        )

        # æ¨¡æ‹Ÿç”¨æˆ·é€‰æ‹©ï¼šä»…è‹±æ–‡ç‰ˆæœ¬
        language_filter = 'english'
        print(f"ğŸ¯ è¯­è¨€è¿‡æ»¤è®¾ç½®: {language_filter}")

        # è®¿é—®ä¸»é¡µé¢
        print(f"\nğŸŒ è®¿é—®ä¸»é¡µé¢...")
        response = downloader.session.get(theme.url)

        if response.status_code == 200:
            print(f"âœ… é¡µé¢è®¿é—®æˆåŠŸ")

            # è§£æé¡µé¢
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')

            # è·å–å­é“¾æ¥
            sub_links = downloader._get_sub_links(soup, theme.url)
            print(f"ğŸ“Š æ‰¾åˆ° {len(sub_links)} ä¸ªæŒ‡å—å­é¡µé¢")

            # åªæµ‹è¯•ç¬¬ä¸€ä¸ªè¯¦æƒ…é¡µ
            test_url = sub_links[0]
            print(f"\nğŸ§ª è¯¦ç»†æµ‹è¯•ç¬¬ä¸€ä¸ªè¯¦æƒ…é¡µ: {test_url}")

            # æ‰‹åŠ¨æ¨¡æ‹ŸPDFè§£æè¿‡ç¨‹
            print(f"\nğŸ” æ‰‹åŠ¨æ¨¡æ‹ŸPDFè§£æå’Œè¿‡æ»¤è¿‡ç¨‹...")

            test_response = downloader.session.get(test_url)
            if test_response.status_code == 200:
                test_soup = BeautifulSoup(test_response.content, 'html.parser')

                # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
                all_links = test_soup.find_all('a', href=True)
                pdf_candidates = []

                for link in all_links:
                    href = link.get('href', '')
                    link_text = link.get_text(strip=True)

                    if href.endswith('.pdf'):
                        pdf_candidates.append({
                            'href': href,
                            'text': link_text,
                            'url': href if href.startswith('http') else 'https://www.nccn.org' + href
                        })

                print(f"ğŸ“‹ æ‰¾åˆ° {len(pdf_candidates)} ä¸ªPDFå€™é€‰é“¾æ¥")

                # æ‰‹åŠ¨åº”ç”¨è¯­è¨€è¿‡æ»¤
                english_pdfs = []
                filtered_pdfs = []

                for pdf in pdf_candidates:
                    # æ£€æµ‹è¯­è¨€
                    detected_lang = downloader._detect_pdf_language(pdf['href'], pdf['text'])
                    should_include = downloader._should_include_pdf(pdf['href'], language_filter, pdf['text'])

                    status = "âœ… ä¿ç•™" if should_include else "âŒ è¿‡æ»¤"
                    print(f"   {detected_lang:10s} | {status} | {pdf['text'][:30]}...")
                    print(f"   {'':12s} | {' '*8} | {pdf['href']}")

                    if should_include:
                        english_pdfs.append(pdf)
                    else:
                        filtered_pdfs.append(pdf)

                print(f"\nğŸ“Š è¿‡æ»¤ç»“æœ:")
                print(f"   æ€»PDFæ•°: {len(pdf_candidates)}")
                print(f"   ä¿ç•™PDFæ•°: {len(english_pdfs)}")
                print(f"   è¿‡æ»¤PDFæ•°: {len(filtered_pdfs)}")

                # éªŒè¯è¿‡æ»¤ç»“æœ
                if len(filtered_pdfs) > 0:
                    print(f"\nâš ï¸ è¢«è¿‡æ»¤çš„PDF:")
                    for pdf in filtered_pdfs:
                        detected_lang = downloader._detect_pdf_language(pdf['href'], pdf['text'])
                        print(f"   âŒ {detected_lang:10s} | {pdf['text'][:40]}...")

                # è°ƒç”¨å®é™…æ–¹æ³•è¿›è¡Œå¯¹æ¯”
                print(f"\nğŸ”„ å¯¹æ¯”ï¼šè°ƒç”¨å®é™…è§£ææ–¹æ³•...")
                actual_pdfs = downloader._get_pdfs_from_detail_page(test_url, "Test Guideline", language_filter)

                print(f"ğŸ“Š æ–¹æ³•è°ƒç”¨ç»“æœ:")
                print(f"   æ‰‹åŠ¨è¿‡æ»¤ç»“æœ: {len(english_pdfs)}")
                print(f"   æ–¹æ³•è°ƒç”¨ç»“æœ: {len(actual_pdfs)}")

                if len(english_pdfs) == len(actual_pdfs):
                    print(f"âœ… ç»“æœä¸€è‡´ï¼Œè¯­è¨€è¿‡æ»¤æ­£å¸¸å·¥ä½œ")
                    return True
                else:
                    print(f"âš ï¸ ç»“æœä¸ä¸€è‡´ï¼Œå­˜åœ¨é—®é¢˜")
                    return False

            else:
                print(f"âŒ æ— æ³•è®¿é—®æµ‹è¯•è¯¦æƒ…é¡µ")
                return False
        else:
            print(f"âŒ ä¸»é¡µé¢è®¿é—®å¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    import json

    success = debug_detailed_filtering()

    print(f"\n{'='*60}")
    if success:
        print("âœ… è¯­è¨€è¿‡æ»¤åŠŸèƒ½æ­£å¸¸")
    else:
        print("âŒ è¯­è¨€è¿‡æ»¤å­˜åœ¨é—®é¢˜")