#!/usr/bin/env python3
"""
æµ‹è¯•æ”¯æŒæ€§æŠ¤ç†æŒ‡å—çš„åŒè¯­æå–åŠŸèƒ½
éªŒè¯ä»Guidelineséƒ¨åˆ†æå–è‹±æ–‡ç‰ˆæœ¬ï¼Œä»Internationaléƒ¨åˆ†æå–ä¸­æ–‡ç‰ˆæœ¬
"""

import sys
import os
import time
import random
import json
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_supportive_care_extraction():
    """æµ‹è¯•æ”¯æŒæ€§æŠ¤ç†æŒ‡å—çš„åŒè¯­æå–åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ”¯æŒæ€§æŠ¤ç†æŒ‡å—çš„åŒè¯­æå–åŠŸèƒ½...")
    print("=" * 60)

    try:
        # è¯»å–é…ç½®æ–‡ä»¶
        config_file = 'config.json'
        with open(config_file, 'r', encoding='utf-8') as f:
            config_data = json.load(f)

        # åˆå§‹åŒ–ä¸‹è½½å™¨
        from download_NCCN_Guide_v2_menu import NCCNDownloaderV2, ThemeConfig

        downloader = NCCNDownloaderV2(config_data)

        # åˆ›å»ºé€‰é¡¹2çš„é…ç½®ï¼ˆæ”¯æŒæ€§æŠ¤ç†æŒ‡å—ï¼‰
        theme = ThemeConfig(
            name='supportive_care',
            display_name='æ”¯æŒæ€§æŠ¤ç†æŒ‡å— (Supportive Care)',
            url='https://www.nccn.org/guidelines/category_3',
            category='supportive_care',
            directory='02_Supportive_Care',
            description='æ”¯æŒæ€§æŠ¤ç†ç›¸å…³æŒ‡å—',
            has_language_filter=True,
            guidelines_only=True  # å¯ç”¨æ–°çš„åŒè¯­æå–
        )

        print(f"ğŸ¯ æµ‹è¯•ä¸»é¢˜: {theme.display_name}")
        print(f"ğŸ“ ä¸‹è½½ç›®å½•: {theme.directory}")
        print(f"ğŸ”— URL: {theme.url}")
        print(f"ğŸ·ï¸  Category: {theme.category}")
        print(f"ğŸŒ è¯­è¨€è¿‡æ»¤: {theme.has_language_filter}")
        print(f"ğŸ¯ æŒ‡å—æå–: {'åŒè¯­æŒ‡å—æå–' if theme.guidelines_only else 'ä¼ ç»Ÿæ–¹æ³•'}")

        # æµ‹è¯•ä¸åŒè¯­è¨€è¿‡æ»¤é€‰é¡¹
        test_cases = [
            ('all', 'å…¨éƒ¨ç‰ˆæœ¬ (è‹±æ–‡ + ä¸­æ–‡)'),
            ('english', 'ä»…è‹±æ–‡ç‰ˆæœ¬'),
            ('chinese', 'ä»…ä¸­æ–‡ç‰ˆæœ¬')
        ]

        for language_filter, description in test_cases:
            print(f"\n{'='*60}")
            print(f"ğŸ¯ æµ‹è¯•è¯­è¨€è¿‡æ»¤: {description}")
            print(f"   language_filter: {language_filter}")

            # è®¿é—®ä¸»é¡µé¢
            print(f"\nğŸŒ è®¿é—®ä¸»é¡µé¢...")
            response = downloader.session.get(theme.url)

            if response.status_code == 200:
                print(f"âœ… é¡µé¢è®¿é—®æˆåŠŸ (çŠ¶æ€ç : {response.status_code})")

                # è§£æé¡µé¢
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.content, 'html.parser')

                # è·å–å­é“¾æ¥ï¼ˆé™åˆ¶æ•°é‡é¿å…é•¿æ—¶é—´è¿è¡Œï¼‰
                print(f"\nğŸ” è·å–guidelines-detailé“¾æ¥...")
                sub_links = downloader._get_sub_links(soup, theme.url)
                print(f"ğŸ“Š æ‰¾åˆ° {len(sub_links)} ä¸ªæŒ‡å—å­é¡µé¢")

                if len(sub_links) > 0:
                    # æµ‹è¯•ç¬¬ä¸€ä¸ªè¯¦æƒ…é¡µ
                    test_link = sub_links[0]
                    print(f"\nğŸ§ª æµ‹è¯•ç¬¬ä¸€ä¸ªè¯¦æƒ…é¡µ: {test_link.split('/')[-1]}")

                    try:
                        # æµ‹è¯•åŒè¯­æå–æ–¹æ³•
                        print(f"   ğŸ” ä½¿ç”¨åŒè¯­æå–æ–¹æ³•...")
                        pdf_links = downloader._get_pdfs_from_detail_page(
                            test_link, f"æ”¯æŒæ€§æŠ¤ç†æŒ‡å—_æµ‹è¯•", language_filter, theme
                        )

                        print(f"   ğŸ“‹ æå–åˆ° {len(pdf_links)} ä¸ªPDF")

                        # ç»Ÿè®¡è¯­è¨€åˆ†å¸ƒ
                        english_count = len([p for p in pdf_links if p['version'] == 'English'])
                        chinese_count = len([p for p in pdf_links if p['version'] == 'Chinese'])

                        print(f"   ğŸ“Š è¯­è¨€åˆ†å¸ƒ:")
                        print(f"      è‹±æ–‡ç‰ˆæœ¬: {english_count}")
                        print(f"      ä¸­æ–‡ç‰ˆæœ¬: {chinese_count}")

                        # æ˜¾ç¤ºæ¯ä¸ªPDFçš„è¯¦ç»†ä¿¡æ¯
                        for j, pdf in enumerate(pdf_links, 1):
                            title = pdf['title'][:50] + "..." if len(pdf['title']) > 50 else pdf['title']
                            version = pdf['version']
                            enhanced_filename = pdf.get('enhanced_filename', 'N/A')
                            print(f"      {j:2d}. {title} (è¯­è¨€: {version})")
                            print(f"          æ–‡ä»¶å: {enhanced_filename}")

                        # éªŒè¯ç»“æœæ˜¯å¦ç¬¦åˆé¢„æœŸ
                        if language_filter == 'all':
                            expected_condition = (english_count > 0 or chinese_count > 0)
                        elif language_filter == 'english':
                            expected_condition = (english_count > 0 and chinese_count == 0)
                        elif language_filter == 'chinese':
                            expected_condition = (chinese_count > 0 and english_count == 0)

                        if expected_condition:
                            print(f"   âœ… è¯­è¨€è¿‡æ»¤éªŒè¯é€šè¿‡")
                        else:
                            print(f"   âš ï¸ è¯­è¨€è¿‡æ»¤éªŒè¯å¤±è´¥")

                        # çŸ­æš‚å»¶è¿Ÿ
                        time.sleep(random.uniform(0.5, 1.5))

                    except Exception as e:
                        print(f"   âŒ å¤„ç†å¤±è´¥: {str(e)}")
                        import traceback
                        traceback.print_exc()
                        continue

                else:
                    print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ°æŒ‡å—å­é¡µé¢é“¾æ¥")
                    return False
            else:
                print(f"âŒ ä¸»é¡µé¢è®¿é—®å¤±è´¥")
                return False

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_bilingual_extraction_logic():
    """æµ‹è¯•åŒè¯­æå–é€»è¾‘"""
    print(f"\nğŸ” æµ‹è¯•åŒè¯­æå–é€»è¾‘:")
    print("=" * 60)

    try:
        from bs4 import BeautifulSoup

        # æ¨¡æ‹ŸHTMLç»“æ„
        mock_html = """
        <div>
            <h4 class="GL">Guidelines</h4>
            <ul class="pdfList">
                <li>
                    <p>
                        <a href="/professionals/physician_gls/pdf/pain.pdf">NCCN Guidelines</a>
                        <span>Version 2.2025</span>
                    </p>
                </li>
            </ul>

            <h4 class="INT">International</h4>
            <div class="international">
                <p>Translations</p>
                <ul class="pdfList">
                    <li>
                        <p>
                            <a href="/professionals/physician_gls/pdf/adult_cancer_pain-chinese1.pdf">Chinese </a>
                            <span>Version 1.2025</span>
                        </p>
                    </li>
                </ul>
            </div>
        </div>
        """

        soup = BeautifulSoup(mock_html, 'html.parser')

        # åˆå§‹åŒ–ä¸‹è½½å™¨
        from download_NCCN_Guide_v2_menu import NCCNDownloaderV2
        import json

        with open('config.json', 'r', encoding='utf-8') as f:
            config_data = json.load(f)

        downloader = NCCNDownloaderV2(config_data)

        print("ğŸ“‹ æ¨¡æ‹ŸHTMLç»“æ„:")
        print("   - Guidelines éƒ¨åˆ†åŒ…å« pain.pdf (è‹±æ–‡)")
        print("   - International éƒ¨åˆ†åŒ…å« adult_cancer_pain-chinese1.pdf (ä¸­æ–‡)")

        # æµ‹è¯•æå–é€»è¾‘
        print(f"\nğŸ§ª æµ‹è¯•æå–é€»è¾‘...")

        # æµ‹è¯•è‹±æ–‡éƒ¨åˆ†æå–
        print(f"   ğŸ” æå–è‹±æ–‡ç‰ˆæœ¬ (Guidelines)...")
        english_pdfs = downloader._extract_from_section(soup, 'GL', 'Guidelines', 'english')
        print(f"   ğŸ“Š è‹±æ–‡PDFæ•°é‡: {len(english_pdfs)}")
        for pdf in english_pdfs:
            print(f"      - {pdf['title']} ({pdf['version']}) -> {pdf['enhanced_filename']}")

        # æµ‹è¯•ä¸­æ–‡éƒ¨åˆ†æå–
        print(f"   ğŸ” æå–ä¸­æ–‡ç‰ˆæœ¬ (International)...")
        chinese_pdfs = downloader._extract_from_section(soup, 'INT', 'International', 'chinese')
        print(f"   ğŸ“Š ä¸­æ–‡PDFæ•°é‡: {len(chinese_pdfs)}")
        for pdf in chinese_pdfs:
            print(f"      - {pdf['title']} ({pdf['version']}) -> {pdf['enhanced_filename']}")

        # æµ‹è¯•åŒè¯­æå–
        print(f"   ğŸ” æµ‹è¯•åŒè¯­æå–...")
        all_pdfs = downloader._extract_bilingual_guidelines(soup, 'all')
        print(f"   ğŸ“Š æ€»PDFæ•°é‡: {len(all_pdfs)}")

        english_count = len([p for p in all_pdfs if p['version'] == 'English'])
        chinese_count = len([p for p in all_pdfs if p['version'] == 'Chinese'])

        print(f"   ğŸ“ˆ è¯­è¨€ç»Ÿè®¡: è‹±æ–‡ {english_count}, ä¸­æ–‡ {chinese_count}")

        if english_count > 0 and chinese_count > 0:
            print(f"   âœ… åŒè¯­æå–éªŒè¯é€šè¿‡")
            return True
        else:
            print(f"   âŒ åŒè¯­æå–éªŒè¯å¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸ¯ æµ‹è¯•æ”¯æŒæ€§æŠ¤ç†æŒ‡å—åŒè¯­æå–åŠŸèƒ½")
    print("éªŒè¯ä»Guidelineså’ŒInternationaléƒ¨åˆ†åˆ†åˆ«æå–è‹±æ–‡å’Œä¸­æ–‡ç‰ˆæœ¬")
    print("=" * 60)

    success1 = test_supportive_care_extraction()
    success2 = test_bilingual_extraction_logic()

    print(f"\n{'='*60}")
    if success1 and success2:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… æ”¯æŒæ€§æŠ¤ç†æŒ‡å—åŒè¯­æå–åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print("âœ… èƒ½å¤Ÿä»Guidelineséƒ¨åˆ†æå–è‹±æ–‡ç‰ˆæœ¬")
        print("âœ… èƒ½å¤Ÿä»Internationaléƒ¨åˆ†æå–ä¸­æ–‡ç‰ˆæœ¬")
        print("âœ… è¯­è¨€è¿‡æ»¤åŠŸèƒ½æ­£å¸¸")
        print("ğŸš€ ç°åœ¨è¿è¡Œ: python download_NCCN_Guide_v2_menu.py")
        print("   é€‰æ‹©é€‰é¡¹2ï¼ŒéªŒè¯åŒè¯­æå–æ•ˆæœ")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        print("ğŸ”§ éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")