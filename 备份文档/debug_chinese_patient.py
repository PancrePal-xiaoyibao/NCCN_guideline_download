#!/usr/bin/env python3
"""
è°ƒè¯•ä¸­æ–‡æ‚£è€…æŒ‡å—é“¾æ¥æ£€æµ‹é—®é¢˜
"""

import sys
import os
from pathlib import Path
from bs4 import BeautifulSoup
import requests

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def debug_chinese_patient_guidelines():
    """è°ƒè¯•ä¸­æ–‡æ‚£è€…æŒ‡å—é“¾æ¥æ£€æµ‹"""
    print("ğŸ” è°ƒè¯•ä¸­æ–‡æ‚£è€…æŒ‡å—é“¾æ¥æ£€æµ‹...")
    print("=" * 60)

    try:
        # è¯»å–Cookie
        with open('extracted_cookies.txt', 'r', encoding='utf-8') as f:
            cookie_string = f.read().strip()

        # è§£æCookie
        cookies = {}
        for item in cookie_string.split(';'):
            if '=' in item:
                key, value = item.strip().split('=', 1)
                cookies[key] = value

        # åˆ›å»ºsession
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        session.cookies.update(cookies)

        # æµ‹è¯•ç”¨æˆ·æåˆ°çš„ä¸­æ–‡ç¿»è¯‘å…¥å£é¡µé¢
        chinese_url = "https://www.nccn.org/global/what-we-do/guidelines-for-patients-translations"
        print(f"ğŸŒ è®¿é—®ä¸­æ–‡ç¿»è¯‘å…¥å£é¡µé¢: {chinese_url}")

        response = session.get(chinese_url)
        print(f"ğŸ“¡ çŠ¶æ€ç : {response.status_code}")

        if response.status_code != 200:
            print("âŒ ä¸­æ–‡ç¿»è¯‘é¡µé¢è®¿é—®å¤±è´¥")
            return

        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')

        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        all_links = soup.find_all('a', href=True)
        print(f"ğŸ”— æ€»é“¾æ¥æ•°: {len(all_links)}")

        # æŸ¥æ‰¾ä¸­æ–‡PDFé“¾æ¥
        chinese_pdfs = []
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)

            # æŸ¥æ‰¾ä¸­æ–‡PDFé“¾æ¥ï¼ˆ-zhæ ‡è¯†ï¼‰
            if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                is_chinese = '-zh' in href.lower()
                if is_chinese:
                    chinese_pdfs.append({
                        'href': href,
                        'text': text,
                        'full_url': 'https://www.nccn.org' + href if href.startswith('/') else href
                    })

        print(f"\nğŸ‡¨ğŸ‡³ æ‰¾åˆ°ä¸­æ–‡PDFé“¾æ¥æ•°é‡: {len(chinese_pdfs)}")
        for i, link in enumerate(chinese_pdfs[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"   {i+1}. {link['text']} -> {link['full_url']}")

        # æµ‹è¯•ç”¨æˆ·æåˆ°çš„å…·ä½“é“¾æ¥
        user_url = "https://www.nccn.org/patients/guidelines/content/PDF/Bladder-zh-patient.pdf"
        print(f"\nğŸ§ª æµ‹è¯•ç”¨æˆ·æåˆ°çš„å…·ä½“é“¾æ¥: {user_url}")

        test_response = session.get(user_url)
        print(f"ğŸ“¡ çŠ¶æ€ç : {test_response.status_code}")

        if test_response.status_code == 200:
            print("âœ… ç”¨æˆ·æåˆ°çš„ä¸­æ–‡PDFé“¾æ¥å¯è®¿é—®")
        else:
            print("âŒ ç”¨æˆ·æåˆ°çš„ä¸­æ–‡PDFé“¾æ¥æ— æ³•è®¿é—®")

        # ç°åœ¨æ£€æŸ¥æ™®é€šæ‚£è€…æŒ‡å—é¡µé¢çš„ä¸­æ–‡é“¾æ¥
        print(f"\nğŸ” æ£€æŸ¥æ™®é€šæ‚£è€…æŒ‡å—é¡µé¢çš„ä¸­æ–‡é“¾æ¥...")
        patient_url = "https://www.nccn.org/patientresources/patient-resources/guidelines-for-patients"

        patient_response = session.get(patient_url)
        print(f"ğŸ“¡ æ‚£è€…æŒ‡å—é¡µé¢çŠ¶æ€ç : {patient_response.status_code}")

        if patient_response.status_code == 200:
            patient_soup = BeautifulSoup(patient_response.content, 'html.parser')
            patient_links = patient_soup.find_all('a', href=True)

            # æŸ¥æ‰¾è¯¦æƒ…é¡µé“¾æ¥å¹¶æµ‹è¯•å‡ ä¸ª
            detail_links = []
            for link in patient_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if '/guidelines-for-patients-details?patientGuidelineId=' in href:
                    detail_url = 'https://www.nccn.org' + href if href.startswith('/') else href
                    detail_links.append({'url': detail_url, 'text': text})

            print(f"ğŸ“‹ æ‰¾åˆ° {len(detail_links)} ä¸ªè¯¦æƒ…é¡µé“¾æ¥ï¼Œæµ‹è¯•å‰3ä¸ª...")

            for i, detail in enumerate(detail_links[:3]):
                try:
                    print(f"\nğŸ“„ æµ‹è¯•è¯¦æƒ…é¡µ {i+1}: {detail['text']}")
                    detail_response = session.get(detail['url'])

                    if detail_response.status_code == 200:
                        detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                        detail_links_page = detail_soup.find_all('a', href=True)

                        chinese_found = 0
                        for link in detail_links_page:
                            href = link.get('href', '')
                            link_text = link.get_text(strip=True)

                            if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                                if '-zh' in href.lower():
                                    chinese_found += 1
                                    print(f"   ğŸ‡¨ğŸ‡³ ä¸­æ–‡PDF: {link_text} -> {href}")

                        print(f"   è¯¦æƒ…é¡µä¸­æ–‡PDFæ•°é‡: {chinese_found}")

                except Exception as e:
                    print(f"   âŒ æµ‹è¯•è¯¦æƒ…é¡µå¤±è´¥: {e}")

    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_chinese_patient_guidelines()