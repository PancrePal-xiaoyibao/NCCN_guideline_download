#!/usr/bin/env python3
"""
æ·±å…¥åˆ†æç¿»è¯‘é¡µé¢ç»“æ„ï¼Œå¯»æ‰¾æ›´å¤šä¸­æ–‡PDF
"""

import sys
import os
from pathlib import Path
from bs4 import BeautifulSoup
import requests

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def analyze_translation_page_structure():
    """æ·±å…¥åˆ†æç¿»è¯‘é¡µé¢ç»“æ„"""
    print("ğŸ” æ·±å…¥åˆ†æç¿»è¯‘é¡µé¢ç»“æ„...")
    print("=" * 60)

    try:
        # è¯»å–Cookie
        with open('extracted_cookies.txt', 'r', encoding='utf-8') as f:
            cookie_string = f.read().strip()

        # è§£æCookie
        cookies = {}
        for item in cookie_string.split(';'):
            if '=' in item:
                key, value = item.strip().split('=', 1)
                cookies[key] = value

        # åˆ›å»ºsession
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        session.cookies.update(cookies)

        # è®¿é—®ç¿»è¯‘é¡µé¢
        translation_url = "https://www.nccn.org/global/what-we-do/guidelines-for-patients-translations"
        print(f"ğŸŒ è®¿é—®ç¿»è¯‘é¡µé¢: {translation_url}")

        response = session.get(translation_url)
        print(f"ğŸ“¡ çŠ¶æ€ç : {response.status_code}")

        if response.status_code != 200:
            print("âŒ ç¿»è¯‘é¡µé¢è®¿é—®å¤±è´¥")
            return

        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')

        print(f"\nğŸ“„ é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'N/A'}")

        # åˆ†æé¡µé¢ç»“æ„
        print(f"\nğŸ” åˆ†æé¡µé¢ç»“æ„...")

        # æŸ¥æ‰¾æ‰€æœ‰h1-h6æ ‡é¢˜
        headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        print(f"\nğŸ“‹ æ‰¾åˆ° {len(headers)} ä¸ªæ ‡é¢˜")
        for header in headers[:20]:  # æ˜¾ç¤ºå‰20ä¸ªæ ‡é¢˜
            print(f"   {header.name}: {header.get_text(strip=True)[:80]}")

        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«"Chinese"çš„æ–‡æœ¬
        chinese_text = soup.get_text()
        if 'Chinese' in chinese_text:
            chinese_sections = []
            lines = chinese_text.split('\n')
            for i, line in enumerate(lines):
                if 'Chinese' in line and line.strip():
                    chinese_sections.append((i, line.strip()))
            print(f"\nğŸ‡¨ğŸ‡³ æ‰¾åˆ° {len(chinese_sections)} è¡ŒåŒ…å«'Chinese'çš„æ–‡æœ¬:")
            for line_num, line_text in chinese_sections[:10]:
                print(f"   è¡Œ{line_num}: {line_text[:100]}...")

        # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
        all_links = soup.find_all('a', href=True)
        pdf_links = []
        zh_links = []

        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)

            if href.endswith('.pdf') and '/patients/guidelines/content/PDF/' in href:
                pdf_links.append({
                    'href': href,
                    'text': text,
                    'url': 'https://www.nccn.org' + href if href.startswith('/') else href
                })

                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡æ ‡è¯†
                if any(keyword in href.lower() or keyword in text.lower() for keyword in ['zh', 'chinese', 'ä¸­æ–‡']):
                    zh_links.append({
                        'href': href,
                        'text': text,
                        'url': 'https://www.nccn.org' + href if href.startswith('/') else href
                    })

        print(f"\nğŸ“„ æ€»PDFé“¾æ¥æ•°: {len(pdf_links)}")
        print(f"ğŸ‡¨ğŸ‡³ ç–‘ä¼¼ä¸­æ–‡PDFé“¾æ¥æ•°: {len(zh_links)}")

        # æ˜¾ç¤ºæ‰€æœ‰PDFé“¾æ¥
        print(f"\nğŸ“‹ æ‰€æœ‰PDFé“¾æ¥:")
        for i, pdf in enumerate(pdf_links, 1):
            print(f"   {i:2d}. {pdf['text'][:60]:<60} -> {pdf['href']}")

        # æŸ¥æ‰¾å¯èƒ½çš„ä¸­æ–‡ç›¸å…³å†…å®¹
        print(f"\nğŸ” æŸ¥æ‰¾å¯èƒ½çš„ä¸­æ–‡ç›¸å…³å†…å®¹...")

        # æŸ¥æ‰¾åŒ…å«-zhçš„é“¾æ¥
        zh_href_count = sum(1 for link in all_links if '-zh' in link.get('href', '').lower())
        print(f"   åŒ…å«'-zh'çš„é“¾æ¥æ•°: {zh_href_count}")

        # æŸ¥æ‰¾åŒ…å«chineseçš„é“¾æ¥
        chinese_text_count = sum(1 for link in all_links if 'chinese' in link.get_text(strip=True).lower())
        print(f"   åŒ…å«'chinese'æ–‡æœ¬çš„é“¾æ¥æ•°: {chinese_text_count}")

        # æŸ¥æ‰¾ä¸­æ–‡æ–‡æœ¬
        chinese_character_links = []
        for link in all_links:
            text = link.get_text(strip=True)
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
            if any(ord(char) > 127 for char in text) and len(text) > 2:
                chinese_character_links.append({
                    'href': link.get('href', ''),
                    'text': text
                })

        print(f"   åŒ…å«ä¸­æ–‡å­—ç¬¦çš„é“¾æ¥æ•°: {len(chinese_character_links)}")
        if chinese_character_links:
            print(f"   å‰5ä¸ªåŒ…å«ä¸­æ–‡å­—ç¬¦çš„é“¾æ¥:")
            for link in chinese_character_links[:5]:
                print(f"      {link['text']} -> {link['href']}")

        # è¯¦ç»†åˆ†ææ¯ä¸ªPDFé“¾æ¥çš„è¯­è¨€
        print(f"\nğŸ” è¯¦ç»†åˆ†æPDFé“¾æ¥è¯­è¨€:")
        language_stats = {'Chinese': 0, 'Spanish': 0, 'English': 0, 'Unknown': 0}

        for pdf in pdf_links:
            href_lower = pdf['href'].lower()
            text_lower = pdf['text'].lower()

            language = 'Unknown'
            if '-zh' in href_lower or 'chinese' in text_lower:
                language = 'Chinese'
            elif '-es' in href_lower or 'spanish' in text_lower:
                language = 'Spanish'
            elif '-en' in href_lower or 'english' in text_lower:
                language = 'English'
            else:
                language = 'English'  # é»˜è®¤è®¤ä¸ºæ˜¯è‹±æ–‡

            language_stats[language] += 1

        print(f"ğŸ“Š è¯­è¨€ç»Ÿè®¡:")
        for lang, count in language_stats.items():
            print(f"   {lang}: {count}")

        return True

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    analyze_translation_page_structure()