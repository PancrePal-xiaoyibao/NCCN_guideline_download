#!/usr/bin/env python3
"""
æµ‹è¯•åŒè¯­æ‚£è€…æŒ‡å—è§£æåŠŸèƒ½
"""

import sys
import os
from pathlib import Path
from bs4 import BeautifulSoup

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class TestBilingualPatientGuidelines:
    def test_parse_main_page(self):
        """æµ‹è¯•ä»ä¸»é¡µé¢ç›´æ¥è§£ææ‚£è€…æŒ‡å—PDFé“¾æ¥"""
        print("ğŸ” æµ‹è¯•åŒè¯­æ‚£è€…æŒ‡å—ä¸»é¡µé¢PDFè§£æ")
        print("=" * 60)

        try:
            # ä»æœ¬åœ°HTMLæ–‡ä»¶æµ‹è¯•
            with open('curl_encn_mainpage_patient_guideline.md', 'r', encoding='utf-8') as f:
                content = f.read()

            # æå–HTMLå†…å®¹
            html_start = content.find('<!DOCTYPE html>')
            if html_start == -1:
                html_start = 0
            html_end = content.rfind('</html>') + 7
            if html_end == 6:
                html_end = len(content)
            html_content = content[html_start:html_end]

            soup = BeautifulSoup(html_content, 'html.parser')

            # æ¨¡æ‹Ÿæ–°çš„è§£æé€»è¾‘
            all_links = soup.find_all('a', href=True)
            pdf_links = []
            found_pdfs = 0

            for link in all_links:
                href = link.get('href', '')

                # æŸ¥æ‰¾æ‚£è€…æŒ‡å—PDFé“¾æ¥ - æ ¹æ®ç”¨æˆ·æä¾›çš„ç»“æ„
                if href.endswith('.pdf') and '/patients/guidelines/content/PDF/' in href:
                    found_pdfs += 1

                    # æ­£ç¡®æ‹¼æ¥URL
                    if href.startswith('http'):
                        pdf_url = href
                    else:
                        from urllib.parse import urljoin
                        base_url = 'https://www.nccn.org'
                        if href.startswith('/'):
                            pdf_url = base_url + href
                        else:
                            pdf_url = urljoin(base_url, href)

                    title = link.text.strip()
                    if not title:
                        # ä»URLæå–æ ‡é¢˜
                        filename = href.split('/')[-1].replace('.pdf', '')
                        if filename.endswith('-zh'):
                            title = filename[:-3].replace('-', ' ') + ' (Chinese)'
                        else:
                            title = filename.replace('-', ' ')

                    # ç¡®å®šç‰ˆæœ¬è¯­è¨€
                    version = 'Chinese' if '-zh' in href.lower() or 'chinese' in href.lower() else 'English'

                    pdf_info = {
                        'title': title,
                        'url': pdf_url,
                        'version': version
                    }

                    pdf_links.append(pdf_info)

                    print(f"âœ… æ‰¾åˆ°PDF: {title} ({version})")
                    print(f"   URL: {pdf_url[:80]}...")
                    print()

            print(f"ğŸ“Š è§£æç»“æœ:")
            print(f"   æ€»PDFæ•°={found_pdfs}")
            print(f"   æˆåŠŸæå–={len(pdf_links)}")

            # æ˜¾ç¤ºè¯­è¨€åˆ†å¸ƒ
            chinese_count = sum(1 for pdf in pdf_links if pdf['version'] == 'Chinese')
            english_count = len(pdf_links) - chinese_count
            print(f"   ä¸­æ–‡ç‰ˆæœ¬={chinese_count}")
            print(f"   è‹±æ–‡ç‰ˆæœ¬={english_count}")

            return len(pdf_links) > 0

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def test_parse_detail_page(self):
        """æµ‹è¯•ä»è¯¦æƒ…é¡µè§£æPDFé“¾æ¥ï¼ˆå¤‡ç”¨æµ‹è¯•ï¼‰"""
        print("\nğŸ” æµ‹è¯•æ‚£è€…æŒ‡å—è¯¦æƒ…é¡µPDFè§£æï¼ˆå¤‡ç”¨ï¼‰")
        print("=" * 60)

        try:
            # ä»æœ¬åœ°HTMLæ–‡ä»¶æµ‹è¯•
            with open('curl_encn_patient_guideline.md', 'r', encoding='utf-8') as f:
                content = f.read()

            # æå–HTMLå†…å®¹
            html_start = content.find('<!DOCTYPE html>')
            if html_start == -1:
                html_start = 0
            html_end = content.rfind('</html>') + 7
            if html_end == 6:
                html_end = len(content)
            html_content = content[html_start:html_end]

            soup = BeautifulSoup(html_content, 'html.parser')

            # æ¨¡æ‹Ÿæ–°çš„è§£æé€»è¾‘
            all_links = soup.find_all('a', href=True)
            pdf_links = []
            found_pdfs = 0

            for link in all_links:
                href = link.get('href', '')

                # æŸ¥æ‰¾æ‚£è€…æŒ‡å—PDFé“¾æ¥
                if href.endswith('.pdf') and '/patients/guidelines/content/PDF/' in href:
                    found_pdfs += 1

                    # æ­£ç¡®æ‹¼æ¥URL
                    if href.startswith('http'):
                        pdf_url = href
                    else:
                        from urllib.parse import urljoin
                        base_url = 'https://www.nccn.org'
                        if href.startswith('/'):
                            pdf_url = base_url + href
                        else:
                            pdf_url = urljoin(base_url, href)

                    title = link.text.strip()
                    if not title:
                        filename = href.split('/')[-1].replace('.pdf', '')
                        if filename.endswith('-zh'):
                            title = filename[:-3].replace('-', ' ') + ' (Chinese)'
                        else:
                            title = filename.replace('-', ' ')

                    # ç¡®å®šç‰ˆæœ¬è¯­è¨€
                    version = 'Chinese' if '-zh' in href.lower() or 'chinese' in href.lower() else 'English'

                    pdf_info = {
                        'title': title,
                        'url': pdf_url,
                        'version': version
                    }

                    pdf_links.append(pdf_info)

                    print(f"ğŸ“„ æ‰¾åˆ°PDF: {title} ({version})")
                    print(f"   URL: {pdf_url[:80]}...")
                    print()

            print(f"ğŸ“Š è¯¦æƒ…é¡µè§£æç»“æœ:")
            print(f"   æ€»PDFæ•°={found_pdfs}")
            print(f"   æˆåŠŸæå–={len(pdf_links)}")

            return len(pdf_links) > 0

        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

def main():
    tester = TestBilingualPatientGuidelines()

    print("ğŸ§ª å¼€å§‹æµ‹è¯•åŒè¯­æ‚£è€…æŒ‡å—åŠŸèƒ½")
    print("=" * 60)

    # æµ‹è¯•ä¸»é¡µé¢è§£æ
    main_page_success = tester.test_parse_main_page()

    # æµ‹è¯•è¯¦æƒ…é¡µè§£æ
    detail_page_success = tester.test_parse_detail_page()

    print(f"\n{'='*60}")
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   ä¸»é¡µé¢è§£æ: {'âœ… é€šè¿‡' if main_page_success else 'âŒ å¤±è´¥'}")
    print(f"   è¯¦æƒ…é¡µè§£æ: {'âœ… é€šè¿‡' if detail_page_success else 'âŒ å¤±è´¥'}")

    if main_page_success and detail_page_success:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åŒè¯­æ‚£è€…æŒ‡å—åŠŸèƒ½å¯ä»¥æ­£å¸¸å·¥ä½œ")
        return True
    else:
        print(f"\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´")
        return False

if __name__ == "__main__":
    main()