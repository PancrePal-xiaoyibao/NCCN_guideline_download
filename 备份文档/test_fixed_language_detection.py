#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„è¯­è¨€æ£€æµ‹é€»è¾‘ï¼ŒéªŒè¯æ˜¯å¦èƒ½è¯†åˆ«æ‰€æœ‰13ä¸ªä¸­æ–‡PDF
"""

import sys
import os
from pathlib import Path
from bs4 import BeautifulSoup
import requests

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_fixed_language_detection():
    """æµ‹è¯•ä¿®å¤åçš„è¯­è¨€æ£€æµ‹é€»è¾‘"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤åçš„è¯­è¨€æ£€æµ‹é€»è¾‘...")
    print("=" * 60)

    try:
        # è¯»å–Cookie
        with open('extracted_cookies.txt', 'r', encoding='utf-8') as f:
            cookie_string = f.read().strip()

        # è§£æCookie
        cookies = {}
        for item in cookie_string.split(';'):
            if '=' in item:
                key, value = item.strip().split('=', 1)
                cookies[key] = value

        # åˆ›å»ºsession
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        session.cookies.update(cookies)

        # è®¿é—®ç¿»è¯‘é¡µé¢
        translation_url = "https://www.nccn.org/global/what-we-do/guidelines-for-patients-translations"
        print(f"ğŸŒ è®¿é—®ç¿»è¯‘é¡µé¢: {translation_url}")

        response = session.get(translation_url)
        print(f"ğŸ“¡ çŠ¶æ€ç : {response.status_code}")

        if response.status_code != 200:
            print("âŒ ç¿»è¯‘é¡µé¢è®¿é—®å¤±è´¥")
            return False

        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        all_links = soup.find_all('a', href=True)

        # å®ç°ä¿®å¤åçš„è¯­è¨€æ£€æµ‹é€»è¾‘
        def _detect_pdf_language(pdf_url: str, link_text: str = "") -> str:
            """ä¿®å¤åçš„è¯­è¨€æ£€æµ‹é€»è¾‘"""
            url_lower = pdf_url.lower()
            text_lower = link_text.lower()

            # æ£€æŸ¥ä¸­æ–‡æ ‡è¯†ï¼ˆæ‰©å±•å¤šç§ä¸­æ–‡æ ‡è¯†ç¬¦ï¼‰
            if any(indicator in url_lower for indicator in ['-zh', '-chi', '-chinese', '-ch(', '-ch)']):
                return 'Chinese'
            elif 'chinese' in text_lower:
                return 'Chinese'
            # æ£€æŸ¥è¥¿ç­ç‰™è¯­æ ‡è¯†
            elif any(indicator in url_lower for indicator in ['-es', '-esl', '-es_', '-spanish', 'spanish']):
                return 'Spanish'
            elif 'spanish' in text_lower:
                return 'Spanish'
            # æ£€æŸ¥å…¶ä»–è¯­è¨€æ ‡è¯†
            elif any(indicator in url_lower for indicator in ['-ar', '-arabic', 'arabic']):
                return 'Arabic'
            elif any(indicator in url_lower for indicator in ['-fr', '-french', 'french']):
                return 'French'
            elif any(indicator in url_lower for indicator in ['-hi', '-hindi', 'hindi']):
                return 'Hindi'
            elif any(indicator in url_lower for indicator in ['-jp', '-japanese', 'japanese']):
                return 'Japanese'
            elif any(indicator in url_lower for indicator in ['-kr', '-korean', 'korean']):
                return 'Korean'
            elif any(indicator in url_lower for indicator in ['-po', '-polish', 'polish']):
                return 'Polish'
            elif any(indicator in url_lower for indicator in ['-pt', '-portuguese', 'portuguese']):
                return 'Portuguese'
            elif any(indicator in url_lower for indicator in ['-ru', '-russian', 'russian']):
                return 'Russian'
            elif any(indicator in url_lower for indicator in ['-vi', '-vietnamese', 'vietnamese']):
                return 'Vietnamese'
            else:
                return 'English'

        # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥å¹¶åˆ†ç±»
        pdf_links = []
        language_stats = {'Chinese': 0, 'Spanish': 48, 'Arabic': 12, 'French': 7, 'Hindi': 10, 'Japanese': 3, 'Korean': 1, 'Polish': 3, 'Portuguese': 8, 'Russian': 3, 'Vietnamese': 2, 'English': 73}

        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)

            # æŸ¥æ‰¾PDFé“¾æ¥
            if href.endswith('.pdf') and '/patients/guidelines/content/PDF/' in href:
                # åº”ç”¨ä¿®å¤åçš„è¯­è¨€æ£€æµ‹
                detected_language = _detect_pdf_language(href, text)

                # æ­£ç¡®æ‹¼æ¥URL
                if href.startswith('http'):
                    pdf_url = href
                else:
                    pdf_url = 'https://www.nccn.org' + href

                pdf_info = {
                    'title': text if text else 'Unknown Title',
                    'url': pdf_url,
                    'href': href,
                    'language': detected_language
                }

                pdf_links.append(pdf_info)

        # ç»Ÿè®¡å„è¯­è¨€PDFæ•°é‡
        actual_stats = {}
        for pdf in pdf_links:
            lang = pdf['language']
            actual_stats[lang] = actual_stats.get(lang, 0) + 1

        print(f"\nğŸ“Š ä¿®å¤åå®é™…è¯­è¨€ç»Ÿè®¡:")
        for lang, count in sorted(actual_stats.items()):
            print(f"   {lang}: {count}")

        # ç‰¹åˆ«æ£€æŸ¥ä¸­æ–‡PDF
        chinese_pdfs = [pdf for pdf in pdf_links if pdf['language'] == 'Chinese']
        print(f"\nğŸ‡¨ğŸ‡³ ä¿®å¤åæ‰¾åˆ°ä¸­æ–‡PDFæ•°é‡: {len(chinese_pdfs)}")

        if len(chinese_pdfs) >= 10:  # ä¸æœŸæœ›çš„13ä¸ªæ¥è¿‘
            print(f"âœ… æˆåŠŸï¼è¯­è¨€æ£€æµ‹é€»è¾‘ä¿®å¤æœ‰æ•ˆ")
            print(f"ğŸ“‹ å‰15ä¸ªä¸­æ–‡PDF:")
            for i, pdf in enumerate(chinese_pdfs[:15], 1):
                print(f"   {i:2d}. {pdf['title'][:60]}")
                print(f"       æ ‡è¯†: {pdf['href'].split('/')[-1]}")
                print()

            if len(chinese_pdfs) > 15:
                print(f"   ... è¿˜æœ‰ {len(chinese_pdfs) - 15} ä¸ªæ–‡ä»¶")

            return True
        else:
            print(f"âš ï¸  ä»æœ‰é—®é¢˜ï¼ŒæœŸæœ›13ä¸ªä¸­æ–‡PDFï¼Œå®é™…æ‰¾åˆ°{len(chinese_pdfs)}ä¸ª")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_fixed_language_detection()

    print(f"\n{'='*60}")
    if success:
        print("ğŸ‰ è¯­è¨€æ£€æµ‹é€»è¾‘ä¿®å¤æˆåŠŸï¼")
        print("âœ… ä¿®å¤åçš„åŒè¯­æ‚£è€…æŒ‡å—è§£æç°åœ¨åº”è¯¥èƒ½æ­£ç¡®å¤„ç†ä¸­æ–‡PDF")
        print("ğŸš€ å‡†å¤‡æµ‹è¯•å®Œæ•´çš„ä¸»ç¨‹åºæµç¨‹")
    else:
        print("âš ï¸  è¯­è¨€æ£€æµ‹é€»è¾‘ä»éœ€è°ƒè¯•")