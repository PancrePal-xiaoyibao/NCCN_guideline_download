#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„åŒè¯­æ‚£è€…æŒ‡å—è§£æé€»è¾‘
"""

import sys
import os
from pathlib import Path
from bs4 import BeautifulSoup
import requests

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_fixed_bilingual_parsing():
    """æµ‹è¯•ä¿®å¤åçš„åŒè¯­æ‚£è€…æŒ‡å—è§£æ"""
    print("ğŸ§ª æµ‹è¯•ä¿®å¤åçš„åŒè¯­æ‚£è€…æŒ‡å—è§£æé€»è¾‘...")
    print("=" * 60)

    try:
        # è¯»å–Cookie
        with open('extracted_cookies.txt', 'r', encoding='utf-8') as f:
            cookie_string = f.read().strip()

        # è§£æCookie
        cookies = {}
        for item in cookie_string.split(';'):
            if '=' in item:
                key, value = item.strip().split('=', 1)
                cookies[key] = value

        # åˆ›å»ºsession
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        session.cookies.update(cookies)

        # è®¿é—®æ‚£è€…æŒ‡å—ä¸»é¡µ
        url = "https://www.nccn.org/patientresources/patient-resources/guidelines-for-patients"
        print(f"ğŸŒ è®¿é—®ä¸»é¡µé¢: {url}")

        response = session.get(url)
        print(f"ğŸ“¡ çŠ¶æ€ç : {response.status_code}")

        if response.status_code != 200:
            print("âŒ ä¸»é¡µé¢è®¿é—®å¤±è´¥")
            return False

        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')

        # æ¨¡æ‹Ÿä¿®å¤åçš„è§£æé€»è¾‘
        print(f"\nğŸ” æ­¥éª¤1: ä»ä¸»é¡µé¢æå–æ‚£è€…æŒ‡å—è¯¦æƒ…é¡µé“¾æ¥...")
        all_links = soup.find_all('a', href=True)
        detail_links = []

        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)

            # æŸ¥æ‰¾è¯¦æƒ…é¡µé“¾æ¥æ ¼å¼
            if '/guidelines-for-patients-details?patientGuidelineId=' in href:
                # æ­£ç¡®æ‹¼æ¥URL
                if href.startswith('http'):
                    detail_url = href
                else:
                    detail_url = 'https://www.nccn.org' + href

                detail_links.append({
                    'url': detail_url,
                    'text': text
                })

        print(f"âœ… æ­¥éª¤1å®Œæˆï¼Œæ‰¾åˆ° {len(detail_links)} ä¸ªæ‚£è€…æŒ‡å—è¯¦æƒ…é¡µ")

        if not detail_links:
            print("âŒ æœªæ‰¾åˆ°æ‚£è€…æŒ‡å—è¯¦æƒ…é¡µé“¾æ¥")
            return False

        # æµ‹è¯•è®¿é—®å‰å‡ ä¸ªè¯¦æƒ…é¡µ
        print(f"\nğŸ” æ­¥éª¤2: éå†è¯¦æƒ…é¡µæå–PDFé“¾æ¥...")
        max_pages = min(3, len(detail_links))  # æµ‹è¯•å‰3ä¸ªè¯¦æƒ…é¡µ
        found_pdfs = []

        for i, detail in enumerate(detail_links[:max_pages]):
            try:
                print(f"ğŸ“„ [{i+1}/{max_pages}] å¤„ç†è¯¦æƒ…é¡µ: {detail['text']}")

                detail_response = session.get(detail['url'])
                if detail_response.status_code != 200:
                    print(f"   âŒ æ— æ³•è®¿é—®è¯¦æƒ…é¡µ: {detail['url']}")
                    continue

                detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                detail_links_page = detail_soup.find_all('a', href=True)

                page_pdfs = 0
                for link in detail_links_page:
                    href = link.get('href', '')
                    link_text = link.get_text(strip=True)

                    # æŸ¥æ‰¾PDFé“¾æ¥
                    if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                        # æ­£ç¡®æ‹¼æ¥URL
                        if href.startswith('http'):
                            pdf_url = href
                        else:
                            pdf_url = 'https://www.nccn.org' + href

                        # ç¡®å®šç‰ˆæœ¬è¯­è¨€
                        version = 'Chinese' if '-zh' in href.lower() or 'chinese' in link_text.lower() else 'English'

                        pdf_info = {
                            'title': link_text if link_text else detail['text'],
                            'url': pdf_url,
                            'version': version,
                            'detail_page': detail['text']
                        }

                        found_pdfs.append(pdf_info)
                        page_pdfs += 1

                        print(f"   ğŸ“„ PDF: {pdf_info['title']} ({version}) -> {pdf_url[:60]}...")

                print(f"   âœ… è¯¦æƒ…é¡µæ‰¾åˆ° {page_pdfs} ä¸ªPDF")

            except Exception as e:
                print(f"   âš ï¸  å¤„ç†è¯¦æƒ…é¡µå¤±è´¥ {detail['text']}: {str(e)}")
                continue

        # ç»Ÿè®¡ç»“æœ
        print(f"\nğŸ“Š è§£æç»“æœç»Ÿè®¡:")
        print(f"   æµ‹è¯•è¯¦æƒ…é¡µæ•°: {max_pages}")
        print(f"   æ€»PDFæ–‡ä»¶æ•°: {len(found_pdfs)}")

        chinese_count = sum(1 for pdf in found_pdfs if pdf['version'] == 'Chinese')
        english_count = len(found_pdfs) - chinese_count
        print(f"   ä¸­æ–‡ç‰ˆæœ¬: {chinese_count}")
        print(f"   è‹±æ–‡ç‰ˆæœ¬: {english_count}")

        if found_pdfs:
            print(f"\nâœ… æµ‹è¯•æˆåŠŸï¼ä¿®å¤åçš„åŒæ­¥éª¤è§£æé€»è¾‘æ­£å¸¸å·¥ä½œ")
            print(f"ğŸ¯ å¯ä»¥æ­£ç¡®ä»è¯¦æƒ…é¡µæå–PDFé“¾æ¥")
            return True
        else:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼šæœªæ‰¾åˆ°PDFæ–‡ä»¶")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_fixed_bilingual_parsing()

    print(f"\n{'='*60}")
    if success:
        print("ğŸ‰ ä¿®å¤éªŒè¯æˆåŠŸï¼")
        print("âœ… åŒè¯­æ‚£è€…æŒ‡å—è§£æé€»è¾‘ç°åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œ")
        print("ğŸš€ å¯ä»¥å°è¯•è¿è¡Œä¸»ç¨‹åºä¸‹è½½åŒè¯­æ‚£è€…æŒ‡å—")
    else:
        print("âš ï¸  ä¿®å¤éªŒè¯å¤±è´¥")
        print("ğŸ”§ éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•è§£æé€»è¾‘")