#!/usr/bin/env python3
"""
ä»”ç»†åˆ†æç¿»è¯‘é¡µé¢ï¼Œå¯»æ‰¾æ‰€æœ‰13ä¸ªä¸­æ–‡PDFçš„å®Œæ•´é“¾æ¥
"""

import sys
import os
from pathlib import Path
from bs4 import BeautifulSoup
import requests

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def debug_detailed_translation_page():
    """è¯¦ç»†è°ƒè¯•ç¿»è¯‘é¡µé¢ï¼Œå¯»æ‰¾æ‰€æœ‰ä¸­æ–‡PDFé“¾æ¥"""
    print("ğŸ” è¯¦ç»†åˆ†æç¿»è¯‘é¡µé¢ï¼Œå¯»æ‰¾æ‰€æœ‰ä¸­æ–‡PDFé“¾æ¥...")
    print("=" * 60)

    try:
        # è¯»å–Cookie
        with open('extracted_cookies.txt', 'r', encoding='utf-8') as f:
            cookie_string = f.read().strip()

        # è§£æCookie
        cookies = {}
        for item in cookie_string.split(';'):
            if '=' in item:
                key, value = item.strip().split('=', 1)
                cookies[key] = value

        # åˆ›å»ºsession
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        session.cookies.update(cookies)

        # ä½¿ç”¨ç”¨æˆ·æä¾›çš„cURLä¸­çš„URL
        translation_url = "https://www.nccn.org/global/what-we-do/guidelines-for-patients-translations"
        print(f"ğŸŒ è®¿é—®ç¿»è¯‘é¡µé¢: {translation_url}")

        response = session.get(translation_url)
        print(f"ğŸ“¡ çŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“„ å“åº”å¤´Content-Type: {response.headers.get('content-type', 'N/A')}")
        print(f"ğŸ“„ å“åº”é•¿åº¦: {len(response.content)} å­—èŠ‚")

        if response.status_code != 200:
            print("âŒ ç¿»è¯‘é¡µé¢è®¿é—®å¤±è´¥")
            return

        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')

        # æŸ¥æ‰¾æ‰€æœ‰h4æ ‡é¢˜ï¼Œç‰¹åˆ«å…³æ³¨Chinese Translations
        print(f"\nğŸ” æŸ¥æ‰¾Chinese Translationséƒ¨åˆ†...")
        headers = soup.find_all('h4')

        chinese_section = None
        for header in headers:
            if 'Chinese' in header.get_text():
                chinese_section = header
                print(f"âœ… æ‰¾åˆ°Chinese Translationséƒ¨åˆ†")
                print(f"   æ ‡é¢˜: {header.get_text(strip=True)}")
                break

        if not chinese_section:
            print("âŒ æœªæ‰¾åˆ°Chinese Translationséƒ¨åˆ†")
            # æ˜¾ç¤ºæ‰€æœ‰h4æ ‡é¢˜
            print("æ‰€æœ‰h4æ ‡é¢˜:")
            for h4 in headers:
                print(f"   {h4.get_text(strip=True)}")
            return

        # ä»Chinese Translationséƒ¨åˆ†å¼€å§‹æŸ¥æ‰¾PDFé“¾æ¥
        print(f"\nğŸ” ä»Chinese Translationséƒ¨åˆ†å¼€å§‹æŸ¥æ‰¾PDFé“¾æ¥...")

        # æ‰¾åˆ°Chinese Translationséƒ¨åˆ†çš„ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ 
        current = chinese_section
        chinese_pdfs = []
        processed_sections = 0

        # éå†Chinese Translationsåé¢çš„æ‰€æœ‰å…ƒç´ ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªh4
        while current and processed_sections < 50:  # é˜²æ­¢æ— é™å¾ªç¯
            current = current.find_next_sibling()

            if current is None:
                break

            if current.name == 'h4':  # é‡åˆ°ä¸‹ä¸€ä¸ªè¯­è¨€éƒ¨åˆ†ï¼Œåœæ­¢
                break

            # æŸ¥æ‰¾å½“å‰å…ƒç´ ä¸­çš„æ‰€æœ‰é“¾æ¥
            links = current.find_all('a', href=True)

            for link in links:
                href = link.get('href', '')
                text = link.get_text(strip=True)

                # æŸ¥æ‰¾PDFé“¾æ¥
                if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                    # æ­£ç¡®æ‹¼æ¥URL
                    if href.startswith('http'):
                        pdf_url = href
                    else:
                        pdf_url = 'https://www.nccn.org' + href

                    chinese_pdfs.append({
                        'title': text,
                        'href': href,
                        'url': pdf_url,
                        'section': current.name if current else 'unknown'
                    })

            processed_sections += 1

        print(f"âœ… ä»Chinese Translationséƒ¨åˆ†æ‰¾åˆ° {len(chinese_pdfs)} ä¸ªPDFé“¾æ¥")

        # æ˜¾ç¤ºæ‰€æœ‰æ‰¾åˆ°çš„ä¸­æ–‡PDF
        print(f"\nğŸ‡¨ğŸ‡³ ä»Chinese Translationséƒ¨åˆ†æ‰¾åˆ°çš„ä¸­æ–‡PDF:")
        for i, pdf in enumerate(chinese_pdfs, 1):
            print(f"   {i:2d}. {pdf['title']}")
            print(f"       æ–‡ä»¶: {pdf['href']}")
            print(f"       å®Œæ•´URL: {pdf['url']}")
            print()

        # ç°åœ¨æŸ¥æ‰¾é¡µé¢ä¸Šæ‰€æœ‰å¯èƒ½çš„PDFé“¾æ¥ï¼Œç­›é€‰ä¸­æ–‡
        print(f"\nğŸ” å…¨é¡µé¢æœç´¢æ‰€æœ‰PDFé“¾æ¥å¹¶ç­›é€‰ä¸­æ–‡...")
        all_links = soup.find_all('a', href=True)
        all_pdfs = []

        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)

            # æŸ¥æ‰¾PDFé“¾æ¥
            if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                all_pdfs.append({
                    'title': text,
                    'href': href,
                    'url': 'https://www.nccn.org' + href if href.startswith('/') else href
                })

        # è¯†åˆ«ä¸­æ–‡PDFçš„å„ç§æ¨¡å¼
        chinese_patterns = [
            '-zh', '-chi', '-chinese', '-ch(', '-ch)',
            'chinese', 'CH', 'zh'
        ]

        all_chinese_pdfs = []
        for pdf in all_pdfs:
            href_lower = pdf['href'].lower()
            text_lower = pdf['title'].lower()

            is_chinese = False
            matched_pattern = None

            for pattern in chinese_patterns:
                if pattern in href_lower or pattern in text_lower:
                    is_chinese = True
                    matched_pattern = pattern
                    break

            if is_chinese:
                all_chinese_pdfs.append({
                    **pdf,
                    'matched_pattern': matched_pattern
                })

        print(f"âœ… å…¨é¡µé¢æœç´¢æ‰¾åˆ° {len(all_chinese_pdfs)} ä¸ªä¸­æ–‡PDF")

        print(f"\nğŸ‡¨ğŸ‡³ å…¨é¡µé¢æœç´¢åˆ°çš„æ‰€æœ‰ä¸­æ–‡PDF:")
        for i, pdf in enumerate(all_chinese_pdfs, 1):
            print(f"   {i:2d}. {pdf['title']}")
            print(f"       åŒ¹é…æ¨¡å¼: {pdf['matched_pattern']}")
            print(f"       æ–‡ä»¶: {pdf['href']}")
            print()

        # éªŒè¯PDFé“¾æ¥çš„å¯è®¿é—®æ€§
        print(f"\nğŸ§ª æµ‹è¯•ä¸­æ–‡PDFé“¾æ¥çš„å¯è®¿é—®æ€§...")
        accessible_count = 0

        for i, pdf in enumerate(all_chinese_pdfs[:5], 1):  # æµ‹è¯•å‰5ä¸ª
            try:
                print(f"ğŸ“„ [{i}/5] æµ‹è¯•: {pdf['title'][:50]}...")

                # ä½¿ç”¨HEADè¯·æ±‚æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
                head_response = session.head(pdf['url'], timeout=10)
                print(f"   ğŸ“¡ HEADçŠ¶æ€ç : {head_response.status_code}")

                if head_response.status_code == 200:
                    accessible_count += 1
                    print(f"   âœ… å¯è®¿é—®")
                else:
                    print(f"   âŒ ä¸å¯è®¿é—®")

            except Exception as e:
                print(f"   âš ï¸  æµ‹è¯•å¤±è´¥: {str(e)}")

        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"   Chinese Translationséƒ¨åˆ†PDFæ•°: {len(chinese_pdfs)}")
        print(f"   å…¨é¡µé¢ä¸­æ–‡PDFæ•°: {len(all_chinese_pdfs)}")
        print(f"   å¯è®¿é—®PDFæ•° (æµ‹è¯•æ ·æœ¬): {accessible_count}/5")

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°äº†æœŸæœ›çš„13ä¸ª
        if len(all_chinese_pdfs) >= 10:
            print(f"\nâœ… æˆåŠŸï¼æ‰¾åˆ° {len(all_chinese_pdfs)} ä¸ªä¸­æ–‡PDF")
            return True
        else:
            print(f"\nâš ï¸  åªæ‰¾åˆ° {len(all_chinese_pdfs)} ä¸ªä¸­æ–‡PDFï¼ŒæœŸæœ›13ä¸ª")
            print(f"   éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•...")
            return False

    except Exception as e:
        print(f"âŒ è¯¦ç»†è°ƒè¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    debug_detailed_translation_page()