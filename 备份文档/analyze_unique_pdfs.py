#!/usr/bin/env python3
"""
åˆ†ææ‚£è€…æŒ‡å—é¡µé¢çš„å”¯ä¸€PDFæ•°é‡
æ£€æŸ¥74ä¸ªè¯¦æƒ…é¡µå®é™…åŒ…å«å¤šå°‘ä¸ªå”¯ä¸€çš„è‹±æ–‡PDF
"""

import sys
import os
import time
import random
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def analyze_unique_pdfs():
    """åˆ†æå”¯ä¸€çš„è‹±æ–‡PDFæ•°é‡"""
    print("ğŸ” åˆ†ææ‚£è€…æŒ‡å—çš„å”¯ä¸€è‹±æ–‡PDFæ•°é‡...")
    print("=" * 60)

    try:
        # è¯»å–é…ç½®æ–‡ä»¶
        config_file = 'config.json'
        with open(config_file, 'r', encoding='utf-8') as f:
            config_data = json.load(f)

        # åˆå§‹åŒ–ä¸‹è½½å™¨
        from download_NCCN_Guide_v2_menu import NCCNDownloaderV2, ThemeConfig

        downloader = NCCNDownloaderV2(config_data)

        # åˆ›å»ºä¸»é¢˜é…ç½®
        theme = ThemeConfig(
            name='patient_guidelines',
            display_name='æ‚£è€…æŒ‡å—è‹±æ–‡ç‰ˆ (Patient Guidelines - English Only)',
            url='https://www.nccn.org/patientresources/patient-resources/guidelines-for-patients',
            category='patient_guidelines_english',
            directory='03_Patient_Guidelines_English',
            description='æ‚£è€…ä¸“ç”¨è‹±æ–‡æŒ‡å—'
        )

        print(f"ğŸ¯ åˆ†æä¸»é¢˜: {theme.display_name}")

        # è®¿é—®ä¸»é¡µé¢
        print(f"\nğŸŒ è®¿é—®ä¸»é¡µé¢...")
        response = downloader.session.get(theme.url)
        if response.status_code != 200:
            print(f"âŒ é¡µé¢è®¿é—®å¤±è´¥")
            return

        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        # è·å–è¯¦æƒ…é¡µé“¾æ¥
        print(f"\nğŸ” è·å–è¯¦æƒ…é¡µé“¾æ¥...")
        sub_links = downloader._get_sub_links_patient_guidelines(soup, theme.url)
        print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(sub_links)} ä¸ªè¯¦æƒ…é¡µé“¾æ¥")

        # åˆ†æå‰10ä¸ªè¯¦æƒ…é¡µçš„PDFæƒ…å†µ
        print(f"\nğŸ§ª åˆ†æå‰10ä¸ªè¯¦æƒ…é¡µçš„PDFæƒ…å†µ...")
        test_links = sub_links[:10]

        total_pdfs = 0
        english_pdfs = 0
        unique_pdfs = set()
        english_unique_pdfs = set()
        pdf_details = []

        for i, sub_url in enumerate(test_links, 1):
            print(f"\nğŸ“„ [{i}/10] åˆ†æ: {sub_url.split('?')[0].split('=')[-1] if '=' in sub_url else 'æœªçŸ¥'}")

            try:
                sub_response = downloader.session.get(sub_url)
                if sub_response.status_code != 200:
                    print(f"   âŒ è®¿é—®å¤±è´¥")
                    continue

                sub_soup = BeautifulSoup(sub_response.content, 'html.parser')

                # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
                page_pdfs = []
                for link in sub_soup.find_all('a', href=True):
                    href = link.get('href', '')
                    link_text = link.get_text(strip=True)

                    if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                        # æ­£ç¡®æ‹¼æ¥URL
                        if href.startswith('http'):
                            pdf_url = href
                        else:
                            pdf_url = 'https://www.nccn.org' + href

                        # æ£€æµ‹è¯­è¨€
                        detected_lang = downloader._detect_pdf_language(href, link_text)

                        pdf_info = {
                            'url': pdf_url,
                            'href': href,
                            'title': link_text,
                            'language': detected_lang,
                            'page_url': sub_url
                        }

                        page_pdfs.append(pdf_info)
                        total_pdfs += 1

                        # æ·»åŠ åˆ°å”¯ä¸€é›†åˆ
                        unique_pdfs.add(pdf_url)

                        if detected_lang in ['English', 'Unknown']:
                            english_pdfs += 1
                            english_unique_pdfs.add(pdf_url)

                print(f"   ğŸ“‹ é¡µé¢PDF: {len(page_pdfs)} ä¸ª")
                print(f"   ğŸ‡ºğŸ‡¸ è‹±æ–‡PDF: {len([p for p in page_pdfs if p['language'] in ['English', 'Unknown']])} ä¸ª")

                # æ˜¾ç¤ºé¡µé¢PDFè¯¦æƒ…
                for pdf in page_pdfs:
                    lang_flag = "ğŸ‡ºğŸ‡¸" if pdf['language'] in ['English', 'Unknown'] else "ğŸŒ"
                    print(f"      {lang_flag} {pdf['title'][:30]}... â†’ {pdf['href']}")

                pdf_details.extend(page_pdfs)

                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(random.uniform(0.5, 1.5))

            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {str(e)}")
                continue

        # ç»Ÿè®¡ç»“æœ
        print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡åˆ†æ:")
        print(f"   æµ‹è¯•è¯¦æƒ…é¡µæ•°: {len(test_links)}")
        print(f"   æ€»PDFå¼•ç”¨æ•°: {total_pdfs}")
        print(f"   è‹±æ–‡PDFå¼•ç”¨æ•°: {english_pdfs}")
        print(f"   å”¯ä¸€PDFæ€»æ•°: {len(unique_pdfs)}")
        print(f"   å”¯ä¸€è‹±æ–‡PDFæ•°: {len(english_unique_pdfs)}")

        # è¯­è¨€åˆ†å¸ƒç»Ÿè®¡
        lang_counts = {}
        for pdf in pdf_details:
            lang = pdf['language']
            lang_counts[lang] = lang_counts.get(lang, 0) + 1

        print(f"\nğŸŒ è¯­è¨€åˆ†å¸ƒ:")
        for lang, count in sorted(lang_counts.items()):
            print(f"   {lang}: {count} ä¸ª")

        # æ˜¾ç¤ºå”¯ä¸€çš„è‹±æ–‡PDF
        print(f"\nğŸ“‹ å”¯ä¸€è‹±æ–‡PDFåˆ—è¡¨:")
        unique_english_list = list(english_unique_pdfs)
        for i, pdf_url in enumerate(sorted(unique_english_list), 1):
            # æå–æ–‡ä»¶å
            filename = pdf_url.split('/')[-1]
            print(f"   {i:2d}. {filename}")

        # è®¡ç®—é¢„æœŸ
        print(f"\nğŸ”® é¢„æœŸåˆ†æ:")
        if len(test_links) > 0:
            pages_per_pdf = len(test_links) / len(english_unique_pdfs) if english_unique_pdfs else 0
            estimated_total_unique = int(len(sub_links) / pages_per_pdf) if pages_per_pdf > 0 else 0

            print(f"   å½“å‰æ ·æœ¬ (10é¡µ) â†’ {len(english_unique_pdfs)} ä¸ªå”¯ä¸€è‹±æ–‡PDF")
            print(f"   å¹³å‡æ¯é¡µPDFæ•°: {pages_per_pdf:.1f}")
            print(f"   é¢„ä¼°æ€»æ•° (74é¡µ): ~{estimated_total_unique} ä¸ªå”¯ä¸€è‹±æ–‡PDF")

            if estimated_total_unique < 20:
                print(f"\nâš ï¸  è­¦å‘Š: é¢„ä¼°çš„å”¯ä¸€è‹±æ–‡PDFæ•°é‡åå°‘")
                print(f"   å¯èƒ½åŸå› :")
                print(f"   1. å¤šä¸ªé¡µé¢å…±äº«ç›¸åŒPDF")
                print(f"   2. æ‚£è€…æŒ‡å—å®é™…ä¸Šç§ç±»è¾ƒå°‘")
                print(f"   3. è§£æé€»è¾‘éœ€è¦è°ƒæ•´")
            else:
                print(f"\nâœ… é¢„ä¼°æ•°é‡åˆç†")
        else:
            print(f"   âš ï¸ æ— æ³•è®¡ç®—é¢„ä¼°ï¼ˆæ²¡æœ‰æˆåŠŸè§£æçš„é¡µé¢ï¼‰")

        return len(english_unique_pdfs)

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return 0

if __name__ == "__main__":
    import json

    unique_count = analyze_unique_pdfs()
    print(f"\n{'='*60}")
    print(f"ğŸ¯ æœ€ç»ˆç»“è®º:")
    print(f"   æµ‹è¯•çš„10ä¸ªè¯¦æƒ…é¡µåŒ…å« {unique_count} ä¸ªå”¯ä¸€è‹±æ–‡PDF")
    print(f"   å¦‚æœè¿™ä¸ªæ¯”ä¾‹ä¿æŒï¼Œ74ä¸ªè¯¦æƒ…é¡µé¢„è®¡æœ‰ ~{unique_count * 7} ä¸ªå”¯ä¸€è‹±æ–‡PDF")
