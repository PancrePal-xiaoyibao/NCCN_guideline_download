#!/usr/bin/env python3
"""
è°ƒè¯•å…·ä½“çš„æ–¹æ³•è¿‡æ»¤é—®é¢˜
æŸ¥çœ‹æ–¹æ³•1å’Œæ–¹æ³•2çš„è¿‡æ»¤æ•ˆæœ
"""

import sys
import os
import time
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def debug_method_filtering():
    """è°ƒè¯•å„ä¸ªæ–¹æ³•çš„è¿‡æ»¤æ•ˆæœ"""
    print("ğŸ” è°ƒè¯•å„ä¸ªæ–¹æ³•çš„è¿‡æ»¤æ•ˆæœ...")
    print("=" * 60)

    try:
        # è¯»å–é…ç½®æ–‡ä»¶
        with open('config.json', 'r', encoding='utf-8') as f:
            config_data = json.load(f)

        # åˆå§‹åŒ–ä¸‹è½½å™¨
        from download_NCCN_Guide_v2_menu import NCCNDownloaderV2, ThemeConfig

        downloader = NCCNDownloaderV2(config_data)

        # æµ‹è¯•URL
        test_url = "https://www.nccn.org/guidelines/guidelines-detail?category=1&id=1410"
        language_filter = 'english'

        print(f"ğŸ¯ æµ‹è¯•URL: {test_url}")
        print(f"ğŸŒ è¯­è¨€è¿‡æ»¤: {language_filter}")

        # è®¿é—®é¡µé¢
        response = downloader.session.get(test_url)
        if response.status_code != 200:
            print(f"âŒ æ— æ³•è®¿é—®é¡µé¢")
            return

        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.content, 'html.parser')

        print(f"\nğŸ” æ‰‹åŠ¨æ¨¡æ‹Ÿæ–¹æ³•1ï¼šæŸ¥æ‰¾ç›´æ¥PDFé“¾æ¥")
        method1_pdfs = []
        all_links = soup.find_all('a', href=True)

        for link in all_links:
            href = link.get('href', '')
            if href.endswith('.pdf'):
                link_text = link.text.strip()

                print(f"   æ£€æŸ¥: {link_text[:30]}...")
                print(f"   URL: {href}")

                # åº”ç”¨è¯­è¨€è¿‡æ»¤
                should_include = downloader._should_include_pdf(href, language_filter, link_text)
                detected_lang = downloader._detect_pdf_language(href, link_text)

                print(f"   æ£€æµ‹è¯­è¨€: {detected_lang}")
                print(f"   åº”è¯¥åŒ…å«: {'âœ…' if should_include else 'âŒ'}")

                if should_include:
                    # æ­£ç¡®æ‹¼æ¥URL
                    if href.startswith('http'):
                        pdf_url = href
                    else:
                        pdf_url = 'https://www.nccn.org' + href

                    method1_pdfs.append({
                        'title': link_text,
                        'url': pdf_url,
                        'version': detected_lang
                    })
                    print(f"   âœ… æ·»åŠ åˆ°æ–¹æ³•1ç»“æœ")

                print()

        print(f"\nğŸ“Š æ–¹æ³•1ç»“æœ: {len(method1_pdfs)} ä¸ªPDF")

        print(f"\nğŸ” æ‰‹åŠ¨æ¨¡æ‹Ÿæ–¹æ³•2ï¼šæŸ¥æ‰¾pdfListé“¾æ¥")
        method2_pdfs = []
        pdf_lists = soup.find_all('ul', class_='pdfList')

        for pdf_list in pdf_lists:
            print(f"ğŸ“‹ æ‰¾åˆ°pdfListåŒºåŸŸ: {len(pdf_list.find_all('a', href=True))} ä¸ªé“¾æ¥")

            for link in pdf_list.find_all('a', href=True):
                href = link.get('href', '')
                if href.endswith('.pdf'):
                    link_text = link.text.strip()

                    print(f"   æ£€æŸ¥: {link_text[:30]}...")
                    print(f"   URL: {href}")

                    # åº”ç”¨è¯­è¨€è¿‡æ»¤
                    should_include = downloader._should_include_pdf(href, language_filter, link_text)
                    detected_lang = downloader._detect_pdf_language(href, link_text)

                    print(f"   æ£€æµ‹è¯­è¨€: {detected_lang}")
                    print(f"   åº”è¯¥åŒ…å«: {'âœ…' if should_include else 'âŒ'}")

                    if should_include:
                        # æ­£ç¡®æ‹¼æ¥URL
                        if href.startswith('http'):
                            pdf_url = href
                        else:
                            pdf_url = 'https://www.nccn.org' + href

                        method2_pdfs.append({
                            'title': link_text,
                            'url': pdf_url,
                            'version': detected_lang
                        })
                        print(f"   âœ… æ·»åŠ åˆ°æ–¹æ³•2ç»“æœ")

                    print()

        print(f"\nğŸ“Š æ–¹æ³•2ç»“æœ: {len(method2_pdfs)} ä¸ªPDF")

        # åˆå¹¶å’Œå»é‡
        all_pdfs = method1_pdfs.copy()
        for pdf in method2_pdfs:
            if not any(p['url'] == pdf['url'] for p in all_pdfs):
                all_pdfs.append(pdf)

        print(f"\nğŸ¯ åˆå¹¶å»é‡åæ€»è®¡: {len(all_pdfs)} ä¸ªPDF")

        print(f"\nğŸ“‹ æœ€ç»ˆåŒ…å«çš„PDF:")
        for i, pdf in enumerate(all_pdfs, 1):
            print(f"   {i:2d}. {pdf['version']:10s} | {pdf['title'][:40]}...")

        return len(all_pdfs)

    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return 0

if __name__ == "__main__":
    import json

    result_count = debug_method_filtering()
    print(f"\n{'='*60}")
    print(f"ğŸ¯ æ‰‹åŠ¨æ–¹æ³•ç»“æœ: {result_count} ä¸ªPDF")
    print(f"ç°åœ¨å¯ä»¥å¯¹æ¯”å®é™…æ–¹æ³•è°ƒç”¨ç»“æœï¼Œçœ‹çœ‹å·®å¼‚åœ¨å“ªé‡Œ")