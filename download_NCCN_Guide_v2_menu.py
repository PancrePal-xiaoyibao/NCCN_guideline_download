#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NCCNæŒ‡å—ä¸‹è½½å·¥å…· v2.0
ä¼˜åŒ–çš„èœå•å¼ä¸‹è½½å·¥å…·ï¼Œæ”¯æŒå¤šä¸»é¢˜åˆ†ç±»ä¸‹è½½ã€å®‰å…¨è¯·æ±‚æ§åˆ¶ã€å®Œå–„æ—¥å¿—å’Œé‡æ–°ä¸‹è½½åŠŸèƒ½

ä½œè€…: Claude Code
ç‰ˆæœ¬: 2.0.0
æ—¥æœŸ: 2024-12-01
"""

import requests
from bs4 import BeautifulSoup
import os
import sys
import time
import json
import random
import logging
import re
from urllib.parse import urlparse, urljoin
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
import hashlib

# ç¬¬ä¸‰æ–¹åº“
try:
    from tqdm import tqdm
except ImportError:
    print("è­¦å‘Š: tqdmæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„è¿›åº¦æ˜¾ç¤º")
    tqdm = lambda x, **kwargs: x


@dataclass
class DownloadStats:
    """ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯"""
    total_files: int = 0
    successful_files: int = 0
    failed_files: int = 0
    skipped_files: int = 0
    total_size_mb: float = 0.0
    downloaded_size_mb: float = 0.0
    start_time: float = 0.0
    end_time: float = 0.0
    failed_files_list: List[str] = None

    def __post_init__(self):
        if self.failed_files_list is None:
            self.failed_files_list = []

    @property
    def duration_seconds(self) -> float:
        """è·å–æ€»è€—æ—¶"""
        if self.end_time and self.start_time:
            return self.end_time - self.start_time
        return 0.0

    @property
    def success_rate(self) -> float:
        """è·å–æˆåŠŸç‡"""
        if self.total_files == 0:
            return 0.0
        return (self.successful_files / self.total_files) * 100

    @property
    def avg_speed_mbps(self) -> float:
        """è·å–å¹³å‡ä¸‹è½½é€Ÿåº¦ (MB/s)"""
        duration = self.duration_seconds
        if duration > 0 and self.downloaded_size_mb > 0:
            return self.downloaded_size_mb / duration
        return 0.0

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        data = asdict(self)
        data['duration_seconds'] = self.duration_seconds
        data['success_rate'] = self.success_rate
        data['avg_speed_mbps'] = self.avg_speed_mbps
        return data


@dataclass
class ThemeConfig:
    """ä¸»é¢˜é…ç½®"""
    name: str
    display_name: str
    url: str
    category: str
    directory: str
    description: str
    has_language_filter: bool = False
    guidelines_only: bool = False


class NCCNDownloaderV2:
    """NCCNä¸‹è½½å™¨ v2.0"""

    # ä¸»é¢˜é…ç½®
    THEMES = {
        '1': ThemeConfig(
            name='cancer_treatment',
            display_name='ç™Œç—‡æ²»ç–—æŒ‡å—è‹±æ–‡ç‰ˆ (Treatment by Cancer Type - English Only)',
            url='https://www.nccn.org/guidelines/category_1',
            category='category_1',
            directory='01_Cancer_Treatment',
            description='æŒ‰ç™Œç—‡ç±»å‹åˆ†ç±»çš„æ²»ç–—æŒ‡å—ï¼ˆè‹±æ–‡ç‰ˆï¼‰',
            has_language_filter=True,
            guidelines_only=True
        ),
        '2': ThemeConfig(
            name='supportive_care',
            display_name='æ”¯æŒæ€§æŠ¤ç†æŒ‡å— (Supportive Care)',
            url='https://www.nccn.org/guidelines/category_3',
            category='category_3',
            directory='02_Supportive_Care',
            description='æ”¯æŒæ€§æŠ¤ç†ç›¸å…³æŒ‡å—',
            has_language_filter=True
        ),
        '3': ThemeConfig(
            name='patient_guidelines',
            display_name='æ‚£è€…æŒ‡å—è‹±æ–‡ç‰ˆ (Patient Guidelines - English Only)',
            url='https://www.nccn.org/patientresources/patient-resources/guidelines-for-patients',
            category='patient_guidelines_english',
            directory='03_Patient_Guidelines_English',
            description='æ‚£è€…ä¸“ç”¨è‹±æ–‡æŒ‡å—'
        ),
        '4': ThemeConfig(
            name='clinical_translations',
            display_name='ä¸´åºŠæŒ‡å—ä¸­æ–‡ç¿»è¯‘ (Clinical Translations)',
            url='https://www.nccn.org/global/what-we-do/clinical-guidelines-translations',
            category='clinical_translations',
            directory='04_Clinical_Translations',
            description='ä¸´åºŠæŒ‡å—ä¸­æ–‡ç¿»è¯‘ç‰ˆæœ¬'
        ),
        '5': ThemeConfig(
            name='patient_translations',
            display_name='æ‚£è€…æŒ‡å—ä¸­æ–‡ç¿»è¯‘ (Patient Guidelines Translations)',
            url='https://www.nccn.org/global/what-we-do/guidelines-for-patients-translations',
            category='patient_translations',
            directory='05_Patient_Translations',
            description='æ‚£è€…æŒ‡å—ä¸­æ–‡ç¿»è¯‘ç‰ˆæœ¬'
        ),
        '6': ThemeConfig(
            name='patient_guidelines_chinese',
            display_name='æ‚£è€…æŒ‡å—ä¸­æ–‡ç‰ˆæœ¬ (Chinese Patient Guidelines)',
            url='https://www.nccn.org/global/what-we-do/guidelines-for-patients-translations',
            category='patient_guidelines_chinese',
            directory='06_Chinese_Patient_Guidelines',
            description='æ‚£è€…æŒ‡å—ä¸­æ–‡ç¿»è¯‘ç‰ˆæœ¬ä¸‹è½½',
            has_language_filter=False
        )
    }

    # è®¤è¯æ–¹å¼
    AUTH_METHODS = {
        '1': 'username_password',
        '2': 'cookie'
    }

    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«è®¤è¯ä¿¡æ¯ç­‰
        """
        self.config = config
        self.session = requests.Session()
        self.setup_session()
        self.base_download_dir = Path('nccn_downloads')
        self.logs_dir = self.base_download_dir / 'logs'
        self.setup_directories()
        self.setup_logging()
        self.stats = DownloadStats()

        # ä¸‹è½½è®¾ç½®
        self.max_retries = 3
        self.retry_delay = 5
        self.request_delay = (2, 5)  # éšæœºå»¶è¿ŸèŒƒå›´
        self.min_file_size = 100 * 1024  # æœ€å°æ–‡ä»¶å¤§å° 100KB

    def setup_session(self):
        """è®¾ç½®ä¼šè¯"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache, no-store, must-revalidate',
            'Pragma': 'no-cache'
        })

        # è®¾ç½®Cookieï¼ˆå¦‚æœä½¿ç”¨Cookieè®¤è¯ï¼‰
        if self.config.get('auth_method') == 'cookie' and self.config.get('cookie'):
            cookie_dict = self.parse_cookie_string(self.config['cookie'])
            self.session.cookies.update(cookie_dict)

    def parse_cookie_string(self, cookie_string: str) -> Dict[str, str]:
        """è§£æCookieå­—ç¬¦ä¸²ä¸ºå­—å…¸æ ¼å¼"""
        cookies = {}
        for item in cookie_string.split(';'):
            if '=' in item:
                key, value = item.strip().split('=', 1)
                cookies[key] = value
        return cookies

    def setup_directories(self):
        """è®¾ç½®ç›®å½•ç»“æ„"""
        # åˆ›å»ºåŸºç¡€ç›®å½•
        self.base_download_dir.mkdir(exist_ok=True)
        self.logs_dir.mkdir(exist_ok=True)

        # åˆ›å»ºå„ä¸»é¢˜ç›®å½•
        for theme in self.THEMES.values():
            theme_dir = self.base_download_dir / theme.directory
            theme_dir.mkdir(exist_ok=True)

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        # æ—¥å¿—æ–‡ä»¶ååŒ…å«æ—¥æœŸ
        log_date = datetime.now().strftime('%Y%m%d')
        log_file = self.logs_dir / f'download_{log_date}.log'

        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )

        self.logger = logging.getLogger(__name__)
        self.logger.info("=== NCCNä¸‹è½½å·¥å…· v2.0 å¯åŠ¨ ===")

    def authenticate(self) -> bool:
        """è®¤è¯åˆ°NCCNç½‘ç«™

        Returns:
            bool: è®¤è¯æ˜¯å¦æˆåŠŸ
        """
        self.logger.info("å¼€å§‹NCCNç½‘ç«™è®¤è¯...")

        try:
            if self.config.get('auth_method') == 'username_password':
                return self._authenticate_username_password()
            elif self.config.get('auth_method') == 'cookie':
                return self._authenticate_cookie()
            else:
                self.logger.error("æœªçŸ¥çš„è®¤è¯æ–¹å¼")
                return False

        except Exception as e:
            self.logger.error(f"è®¤è¯è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False

    def _authenticate_username_password(self) -> bool:
        """ç”¨æˆ·åå¯†ç è®¤è¯"""
        email = self.config.get('username')
        password = self.config.get('password')

        if not email or not password:
            self.logger.error("ç”¨æˆ·åæˆ–å¯†ç æœªè®¾ç½®")
            return False

        try:
            # 1. è®¿é—®ç™»å½•é¡µé¢è·å–token
            login_page_url = "https://www.nccn.org/login"
            self.logger.debug(f"è®¿é—®ç™»å½•é¡µé¢: {login_page_url}")

            response = self.session.get(login_page_url)
            response.raise_for_status()

            # è§£æé¡µé¢è·å–token
            soup = BeautifulSoup(response.content, 'html.parser')
            token_element = soup.find('input', {'name': '__RequestVerificationToken'})
            token = token_element.get('value', '') if token_element else ''

            # 2. æ„å»ºç™»å½•æ•°æ®
            login_data = {
                '__RequestVerificationToken': token,
                'Username': email,
                'Password': password,
                'RememberMe': 'true'
            }

            # 3. å‘é€ç™»å½•è¯·æ±‚
            self.logger.debug("å‘é€ç™»å½•è¯·æ±‚...")
            login_response = self.session.post(
                login_page_url,
                data=login_data,
                allow_redirects=True
            )
            login_response.raise_for_status()

            # 4. éªŒè¯ç™»å½•çŠ¶æ€
            if 'login' in login_response.url.lower():
                self.logger.error("ç™»å½•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç ")
                return False

            # 5. æµ‹è¯•è®¿é—®å—é™é¡µé¢
            test_url = "https://www.nccn.org/guidelines/category_1"
            test_response = self.session.get(test_url)

            if test_response.status_code == 200 and 'login' not in test_response.url.lower():
                self.logger.info("ç”¨æˆ·åå¯†ç è®¤è¯æˆåŠŸ")
                return True
            else:
                self.logger.error("ç™»å½•çŠ¶æ€éªŒè¯å¤±è´¥")
                return False

        except Exception as e:
            self.logger.error(f"ç”¨æˆ·åå¯†ç è®¤è¯å¤±è´¥: {str(e)}")
            return False

    def _authenticate_cookie(self) -> bool:
        """Cookieè®¤è¯"""
        try:
            # è·å–Cookieæ–‡ä»¶è·¯å¾„
            cookie_file = self.config.get('cookie_file', 'extracted_cookies.txt')

            if not os.path.exists(cookie_file):
                self.logger.error(f"Cookieæ–‡ä»¶ä¸å­˜åœ¨: {cookie_file}")
                return False

            # è¯»å–Cookieæ–‡ä»¶
            try:
                with open(cookie_file, 'r', encoding='utf-8') as f:
                    cookie_string = f.read().strip()
            except Exception as e:
                self.logger.error(f"è¯»å–Cookieæ–‡ä»¶å¤±è´¥: {str(e)}")
                return False

            if not cookie_string:
                self.logger.error("Cookieæ–‡ä»¶ä¸ºç©º")
                return False

            # è§£æCookieå­—ç¬¦ä¸²
            try:
                cookies = {}
                for item in cookie_string.split(';'):
                    if '=' in item:
                        key, value = item.strip().split('=', 1)
                        cookies[key] = value

                # æ·»åŠ Cookieåˆ°session
                self.session.cookies.update(cookies)
                self.logger.info(f"æˆåŠŸåŠ è½½ {len(cookies)} ä¸ªCookie")

            except Exception as e:
                self.logger.error(f"è§£æCookieå¤±è´¥: {str(e)}")
                return False

            # æµ‹è¯•è®¿é—®å—é™é¡µé¢
            test_url = "https://www.nccn.org/guidelines/category_1"
            self.logger.debug(f"ä½¿ç”¨Cookieæµ‹è¯•è®¿é—®: {test_url}")

            response = self.session.get(test_url)

            if response.status_code == 200 and 'login' not in response.url.lower():
                self.logger.info("Cookieè®¤è¯æˆåŠŸ")
                return True
            else:
                self.logger.error("Cookieè®¤è¯å¤±è´¥ï¼Œå¯èƒ½éœ€è¦é‡æ–°è·å–")
                return False

        except Exception as e:
            self.logger.error(f"Cookieè®¤è¯å¤±è´¥: {str(e)}")
            return False

    def ensure_authenticated(self) -> bool:
        """ç¡®ä¿å·²è®¤è¯"""
        try:
            test_url = "https://www.nccn.org/guidelines/category_1"
            response = self.session.get(test_url)

            if 'login' in response.url.lower():
                self.logger.warning("æ£€æµ‹åˆ°ç™»å½•çŠ¶æ€å¤±æ•ˆï¼Œå°è¯•é‡æ–°è®¤è¯")
                return self.authenticate()
            return True

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥è®¤è¯çŠ¶æ€å¤±è´¥: {str(e)}")
            return self.authenticate()

    def download_theme(self, theme_key: str, language_filter: str = 'all') -> bool:
        """ä¸‹è½½æŒ‡å®šä¸»é¢˜çš„æŒ‡å—

        Args:
            theme_key: ä¸»é¢˜é”® ('1'-'5')
            language_filter: è¯­è¨€è¿‡æ»¤é€‰é¡¹ ('all', 'english', 'chinese')

        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        if theme_key not in self.THEMES:
            self.logger.error(f"æ— æ•ˆçš„ä¸»é¢˜é”®: {theme_key}")
            return False

        theme = self.THEMES[theme_key]
        self.logger.info(f"å¼€å§‹ä¸‹è½½ä¸»é¢˜: {theme.display_name}")

        # ç¡®ä¿è®¤è¯
        if not self.ensure_authenticated():
            self.logger.error("è®¤è¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ä¸‹è½½")
            return False

        # åˆå§‹åŒ–ç»Ÿè®¡
        self.stats = DownloadStats()
        self.stats.start_time = time.time()

        try:
            # è·å–PDFé“¾æ¥
            pdf_links = self._get_pdf_links(theme, language_filter)
            if not pdf_links:
                self.logger.warning(f"æœªæ‰¾åˆ°PDFé“¾æ¥: {theme.display_name}")
                return False

            self.stats.total_files = len(pdf_links)
            self.logger.info(f"æ‰¾åˆ° {self.stats.total_files} ä¸ªPDFæ–‡ä»¶")

            # åˆ›å»ºä¸»é¢˜ç›®å½•
            theme_dir = self.base_download_dir / theme.directory
            theme_dir.mkdir(exist_ok=True)

            # ä¸‹è½½æ¯ä¸ªPDF
            successful_count = 0
            failed_files = []

            for i, pdf_info in enumerate(pdf_links, 1):
                self.logger.info(f"[{i}/{self.stats.total_files}] å¤„ç†: {pdf_info['title']}")

                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(random.uniform(*self.request_delay))

                # ä¸‹è½½æ–‡ä»¶
                success = self._download_single_pdf(pdf_info, theme_dir)

                if success:
                    successful_count += 1
                    self.stats.successful_files += 1
                else:
                    failed_files.append(pdf_info['title'])
                    self.stats.failed_files += 1

            self.stats.end_time = time.time()
            self.stats.failed_files_list = failed_files

            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            self._generate_download_report(theme)

            # è¯¢é—®æ˜¯å¦é‡æ–°ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶
            if failed_files:
                self._handle_failed_downloads(failed_files, theme_dir)

            return successful_count > 0

        except Exception as e:
            self.logger.error(f"ä¸‹è½½ä¸»é¢˜å¤±è´¥: {str(e)}")
            return False

    def _get_pdf_links(self, theme: ThemeConfig, language_filter: str = 'all') -> List[Dict[str, Any]]:
        """è·å–PDFé“¾æ¥åˆ—è¡¨

        Args:
            theme: ä¸»é¢˜é…ç½®
            language_filter: è¯­è¨€è¿‡æ»¤é€‰é¡¹ ('all', 'english', 'chinese')

        Returns:
            List[Dict]: PDFä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«title, url, versionç­‰
        """
        try:
            self.logger.info(f"ğŸ” å¼€å§‹è·å–PDFé“¾æ¥: {theme.url}")
            self.logger.debug(f"è¯·æ±‚ä¸»é¢˜: {theme.display_name}")

            # ç¬¬ä¸€æ­¥ï¼šè·å–ä¸»é¡µé¢
            self.logger.info(f"ğŸ“¡ å‘é€HTTPè¯·æ±‚...")
            response = self.session.get(theme.url, timeout=30)
            response.raise_for_status()

            self.logger.info(f"âœ… HTTPè¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            self.logger.info(f"ğŸ“„ é¡µé¢å†…å®¹é•¿åº¦: {len(response.content)} å­—èŠ‚")

            soup = BeautifulSoup(response.content, 'html.parser')

            # æ ¹æ®ä¸»é¢˜ç±»å‹ä½¿ç”¨ä¸åŒçš„è§£æç­–ç•¥
            if theme.category in ['clinical_translations', 'patient_translations']:
                # ä¸­æ–‡ç¿»è¯‘é¡µé¢ï¼šç›´æ¥è§£æPDFé“¾æ¥ï¼Œæ— éœ€ä¸¤æ­¥æµç¨‹
                self.logger.info(f"ğŸ¯ ä½¿ç”¨è§£æç­–ç•¥: translations (ç¿»è¯‘ç‰ˆæœ¬ï¼Œç›´æ¥è§£æ)")
                pdf_links = self._parse_translations(soup, theme, language_filter)
            elif theme.category == 'patient_guidelines_chinese':
                # æ‚£è€…æŒ‡å—ä¸­æ–‡ç‰ˆæœ¬ï¼šç›´æ¥è§£æç¿»è¯‘é¡µé¢
                self.logger.info(f"ğŸ¯ ä½¿ç”¨è§£æç­–ç•¥: chinese_only (æ‚£è€…æŒ‡å—ä¸­æ–‡ç‰ˆæœ¬ï¼Œç›´æ¥è®¿é—®ç¿»è¯‘é¡µé¢)")
                pdf_links = self._parse_patient_guidelines_chinese(soup, theme)
            elif theme.category == 'patient_guidelines_english':
                # æ‚£è€…æŒ‡å—è‹±æ–‡ç‰ˆæœ¬ï¼šåŒæ­¥éª¤è§£æï¼Œåªæå–è‹±æ–‡PDF
                self.logger.info(f"ğŸ¯ ä½¿ç”¨è§£æç­–ç•¥: english_only (æ‚£è€…æŒ‡å—è‹±æ–‡ç‰ˆæœ¬ï¼ŒåŒæ­¥è§£æ)")
                pdf_links = self._parse_patient_guidelines_english(soup, theme, language_filter)
            else:
                # æ ‡å‡†é¡µé¢ï¼šä¸¤æ­¥æµç¨‹
                self.logger.info(f"ğŸ” è§£æä¸»é¡µé¢ï¼Œè·å–guidelines-detailé“¾æ¥...")
                sub_links = self._get_sub_links(soup, theme.url)

                if not sub_links:
                    self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å­é“¾æ¥")
                    return []

                self.logger.info(f"ğŸ“Š æ‰¾åˆ° {len(sub_links)} ä¸ªæŒ‡å—å­é¡µé¢")

                # éå†æ¯ä¸ªå­é“¾æ¥ï¼Œè·å–PDFé“¾æ¥
                pdf_links = []
                for i, sub_url in enumerate(sub_links, 1):
                    self.logger.info(f"ğŸ“„ [{i}/{len(sub_links)}] æ­£åœ¨å¤„ç†: {sub_url}")

                    # è·å–å­é¡µé¢çš„PDFé“¾æ¥
                    sub_pdf_links = self._get_pdfs_from_detail_page(sub_url, f"æŒ‡å—_{i}", language_filter, theme)
                    pdf_links.extend(sub_pdf_links)

                    self.logger.info(f"ğŸ“ˆ å½“å‰ç´¯è®¡PDFæ•°é‡: {len(pdf_links)}")

                    # è¯·æ±‚é—´éš”
                    if i < len(sub_links):
                        time.sleep(random.uniform(*self.request_delay))

            self.logger.info(f"ğŸ¯ è§£æå®Œæˆï¼Œæ€»å…±æ‰¾åˆ° {len(pdf_links)} ä¸ªPDFæ–‡ä»¶")
            return pdf_links

        except requests.exceptions.Timeout:
            self.logger.error(f"â° è¯·æ±‚è¶…æ—¶: {theme.url}")
            return []
        except requests.exceptions.ConnectionError:
            self.logger.error(f"ğŸ”Œ è¿æ¥é”™è¯¯: {theme.url}")
            return []
        except Exception as e:
            self.logger.error(f"âŒ è·å–PDFé“¾æ¥å¤±è´¥: {str(e)}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []

    def _get_sub_links_patient_guidelines(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """è·å–æ‚£è€…æŒ‡å—é¡µé¢çš„guidelines-detailé“¾æ¥

        Args:
            soup: BeautifulSoupå¯¹è±¡
            base_url: åŸºç¡€URL

        Returns:
            List[str]: æ‚£è€…æŒ‡å—è¯¦æƒ…é¡µé“¾æ¥URLåˆ—è¡¨
        """
        sub_links = []

        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«guidelines-for-patients-detailsçš„é“¾æ¥
            all_links = soup.find_all('a', href=True)
            guidelines_links = [link for link in all_links if '/guidelines-for-patients-details?' in link.get('href', '')]

            for link in guidelines_links:
                href = link['href']
                full_url = urljoin(base_url, href)
                if full_url not in sub_links:
                    sub_links.append(full_url)
                    self.logger.debug(f"æ‰¾åˆ°æ‚£è€…æŒ‡å—é“¾æ¥: {full_url}")

            return sub_links

        except Exception as e:
            self.logger.error(f"è·å–æ‚£è€…æŒ‡å—å­é“¾æ¥å¤±è´¥: {str(e)}")
            return []

    def _get_sub_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """è·å–æ‰€æœ‰guidelines-detailå­é“¾æ¥

        Args:
            soup: BeautifulSoupå¯¹è±¡
            base_url: åŸºç¡€URL

        Returns:
            List[str]: å­é“¾æ¥URLåˆ—è¡¨
        """
        sub_links = []

        try:
            # æŸ¥æ‰¾guideline-itemsåŒºåŸŸ
            guideline_items = soup.find('div', class_='guideline-items')
            if guideline_items:
                # æŸ¥æ‰¾æ‰€æœ‰item-nameä¸‹çš„é“¾æ¥
                for item in guideline_items.find_all('div', class_='item-name'):
                    link = item.find('a')
                    if link and link.get('href'):
                        href = link['href']
                        if '/guidelines/guidelines-detail' in href:
                            full_url = urljoin(base_url, href)
                            sub_links.append(full_url)
                            self.logger.debug(f"æ‰¾åˆ°æŒ‡å—é“¾æ¥: {full_url}")

            if not sub_links:
                # å¤‡ç”¨ç­–ç•¥ï¼šç›´æ¥æŸ¥æ‰¾æ‰€æœ‰åŒ…å«guidelines-detailçš„é“¾æ¥
                all_links = soup.find_all('a', href=True)
                guidelines_links = [link for link in all_links if '/guidelines/guidelines-detail' in link.get('href', '')]

                for link in guidelines_links:
                    href = link['href']
                    full_url = urljoin(base_url, href)
                    if full_url not in sub_links:
                        sub_links.append(full_url)

            return sub_links

        except Exception as e:
            self.logger.error(f"è·å–å­é“¾æ¥å¤±è´¥: {str(e)}")
            return []

    def _parse_category_1(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """è§£æç™Œç—‡æ²»ç–—æŒ‡å—é¡µé¢"""
        pdf_links = []

        # æŸ¥æ‰¾guideline-itemsåŒºåŸŸ
        guideline_items = soup.find('div', class_='guideline-items')
        if guideline_items:
            # æŸ¥æ‰¾æ‰€æœ‰item-nameä¸‹çš„é“¾æ¥
            for item in guideline_items.find_all('div', class_='item-name'):
                link = item.find('a')
                if link and link.get('href'):
                    href = link['href']
                    if '/guidelines/guidelines-detail' in href:
                        # è·å–å­é¡µé¢çš„PDF
                        full_url = urljoin('https://www.nccn.org', href)
                        title = link.text.strip()

                        self.logger.debug(f"æ‰¾åˆ°æŒ‡å—é¡µé¢: {title} - {full_url}")

                        # è·å–å­é¡µé¢çš„PDFé“¾æ¥
                        sub_pdf_links = self._get_pdfs_from_detail_page(full_url, title, 'all', None)
                        pdf_links.extend(sub_pdf_links)

        return pdf_links

    def _parse_category_3(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """è§£ææ”¯æŒæ€§æŠ¤ç†æŒ‡å—é¡µé¢"""
        return self._parse_category_1(soup)  # ä½¿ç”¨ç›¸åŒçš„è§£æé€»è¾‘

    def _parse_patient_resources(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """è§£ææ‚£è€…èµ„æºé¡µé¢"""
        pdf_links = []

        # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.endswith('.pdf'):
                pdf_url = urljoin('https://www.nccn.org', href)
                title = link.text.strip()
                if not title:
                    title = href.split('/')[-1].split('.')[0]

                pdf_links.append({
                    'title': title,
                    'url': pdf_url,
                    'version': 'Latest'
                })

        return pdf_links

    def _parse_translations(self, soup: BeautifulSoup, theme: ThemeConfig, language_filter: str = 'all') -> List[Dict[str, Any]]:
        """è§£æç¿»è¯‘æŒ‡å—é¡µé¢ - ä¸“é—¨æå–Chinese Translationséƒ¨åˆ†çš„PDFé“¾æ¥"""
        pdf_links = []

        self.logger.info(f"ğŸ” å¼€å§‹è§£æç¿»è¯‘é¡µé¢PDFé“¾æ¥...")
        self.logger.info(f"ğŸŒ è¯­è¨€è¿‡æ»¤: {language_filter}")
        self.logger.info(f"ğŸ¯ ç›®æ ‡: ä¸“é—¨æå–Chinese Translationséƒ¨åˆ†çš„PDFé“¾æ¥")

        # æŸ¥æ‰¾Chinese Translationséƒ¨åˆ†
        chinese_section = None
        chinese_headings = soup.find_all(['h1', 'h2', 'h3', 'h4'], string=lambda text: text and 'Chinese' in text and 'Translation' in text)

        if not chinese_headings:
            chinese_headings = soup.find_all(['h1', 'h2', 'h3', 'h4'], string=lambda text: text and 'Chinese' in text)

        for heading in chinese_headings:
            self.logger.info(f"ğŸ” æ‰¾åˆ°æ ‡é¢˜: {heading.get_text().strip()}")

            # æŸ¥æ‰¾æ ‡é¢˜åçš„pdfList
            current = heading.next_sibling
            while current:
                if hasattr(current, 'name') and current.name == 'ul' and 'pdfList' in current.get('class', []):
                    chinese_section = current
                    self.logger.info(f"âœ… æ‰¾åˆ°Chinese PDFåˆ—è¡¨")
                    break
                elif hasattr(current, 'name') and current.name in ['h1', 'h2', 'h3', 'h4']:
                    # é‡åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ï¼Œåœæ­¢æœç´¢
                    break
                current = current.next_sibling

            if chinese_section:
                break

        if not chinese_section:
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°Chinese Translationséƒ¨åˆ†ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•")
            # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥æŸ¥æ‰¾åŒ…å«'chinese'çš„PDFé“¾æ¥
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '').lower()
                if 'chinese' in href and href.endswith('.pdf'):
                    self.logger.info(f"ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æ‰¾åˆ°ä¸­æ–‡PDF: {href}")
                    chinese_section = link.find_parent('ul', class_='pdfList')
                    if chinese_section:
                        break

        if not chinese_section:
            self.logger.error(f"âŒ æ— æ³•æ‰¾åˆ°Chinese Translationséƒ¨åˆ†çš„PDFåˆ—è¡¨")
            return []

        # ä»Chinese Translationséƒ¨åˆ†æå–PDFé“¾æ¥
        pdf_count = 0
        links = chinese_section.find_all('a', href=True)

        for link in links:
            href = link.get('href', '')
            if href.endswith('.pdf'):
                # å¯¹äºChinese Translationséƒ¨åˆ†ï¼Œå¿½ç•¥è¯­è¨€è¿‡æ»¤ï¼ˆå› ä¸ºè¿™é‡Œæœ¬èº«å°±æ˜¯ä¸­æ–‡ï¼‰
                if language_filter == 'english':
                    # å¦‚æœåªè¦æ±‚è‹±æ–‡ï¼Œè·³è¿‡ä¸­æ–‡ç¿»è¯‘
                    continue

                pdf_count += 1

                # æ­£ç¡®æ‹¼æ¥URL - ä½¿ç”¨NCCNæ ¹åŸŸå
                if href.startswith('http'):
                    pdf_url = href
                else:
                    base_url = 'https://www.nccn.org'
                    if href.startswith('/'):
                        pdf_url = base_url + href
                    else:
                        pdf_url = urljoin(base_url, href)

                title = link.text.strip()
                if not title:
                    title = href.split('/')[-1].split('.')[0]

                # æå–ç‰ˆæœ¬ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                version_span = link.find_next('span', string=lambda text: text and 'Version' in text)
                version = version_span.get_text().strip() if version_span else 'Latest'

                pdf_links.append({
                    'title': title,
                    'url': pdf_url,
                    'version': 'Chinese',
                    'directory': theme.directory
                })

                if pdf_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    self.logger.info(f"ğŸ“„ æ‰¾åˆ°ä¸­æ–‡PDF: {title} (v: {version}) -> {pdf_url[:60]}...")

        self.logger.info(f"âœ… Chinese Translationsè§£æå®Œæˆï¼Œå…±æ‰¾åˆ° {pdf_count} ä¸ªä¸­æ–‡PDFé“¾æ¥")
        return pdf_links

    def _extract_guidelines_only(self, soup: BeautifulSoup, language_filter: str = 'all') -> List[Dict[str, Any]]:
        """ä¸“é—¨æå–"Guidelines"éƒ¨åˆ†çš„æ ¸å¿ƒæŒ‡å—PDFï¼Œå¿½ç•¥å…¶ä»–é™„åŠ æ–‡ä»¶"""
        pdf_links = []

        self.logger.info(f"ğŸ¯ ä¸“é—¨æå–Guidelineséƒ¨åˆ†çš„æ ¸å¿ƒæŒ‡å—...")

        try:
            # æŸ¥æ‰¾æ‰€æœ‰h4æ ‡ç­¾ï¼Œå¯»æ‰¾"Guidelines"
            headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            guidelines_section = None

            for header in headers:
                header_text = header.get_text(strip=True)
                if 'Guidelines' in header_text and ('GL' in header.get('class', []) or 'guidelines' in header_text.lower()):
                    guidelines_section = header
                    self.logger.info(f"âœ… æ‰¾åˆ°Guidelineséƒ¨åˆ†: {header_text}")
                    break

            if not guidelines_section:
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°Guidelineséƒ¨åˆ†")
                return []

            # æ‰¾åˆ°Guidelineséƒ¨åˆ†åçš„pdfList
            current = guidelines_section
            pdf_list_found = False

            # éå†Guidelinesåé¢çš„å…ƒç´ ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªpdfList
            while current and not pdf_list_found:
                current = current.find_next_sibling()

                if current is None:
                    break

                # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ï¼Œåœæ­¢
                if current.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    self.logger.info(f"ğŸ›‘ é‡åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜: {current.get_text(strip=True)[:50]}...")
                    break

                # æŸ¥æ‰¾pdfList
                if current.name == 'ul' and 'pdfList' in current.get('class', []):
                    pdf_list_found = True
                    self.logger.info(f"ğŸ“‹ æ‰¾åˆ°Guidelinesä¸‹çš„pdfListï¼ŒåŒ…å« {len(current.find_all('a', href=True))} ä¸ªé“¾æ¥")

                    # æå–pdfListä¸­çš„PDFé“¾æ¥
                    for link in current.find_all('a', href=True):
                        href = link.get('href', '')
                        if href.endswith('.pdf'):
                            # åº”ç”¨è¯­è¨€è¿‡æ»¤
                            link_text = link.text.strip()
                            if not self._should_include_pdf(href, language_filter, link_text):
                                self.logger.debug(f"ğŸ” è·³è¿‡éè‹±æ–‡PDF: {link_text}")
                                continue

                            # æå–ç‰ˆæœ¬ä¿¡æ¯
                            version_info = self._extract_version_info(link)

                            # æ­£ç¡®æ‹¼æ¥URL
                            if href.startswith('http'):
                                pdf_url = href
                            else:
                                base_url = 'https://www.nccn.org'
                                if href.startswith('/'):
                                    pdf_url = base_url + href
                                else:
                                    pdf_url = urljoin(base_url, href)

                            # ç”Ÿæˆå¢å¼ºçš„æ ‡é¢˜å’Œç‰ˆæœ¬ä¿¡æ¯
                            enhanced_info = self._enhance_pdf_info(link_text, version_info, pdf_url)

                            pdf_links.append({
                                'title': enhanced_info['title'],
                                'url': pdf_url,
                                'version': enhanced_info['version'],
                                'filename': enhanced_info['filename'],  # å¸¦ç‰ˆæœ¬çš„æ–‡ä»¶å
                                'is_guideline': True  # æ ‡è®°ä¸ºæ ¸å¿ƒæŒ‡å—
                            })

                            self.logger.info(f"ğŸ“„ æ ¸å¿ƒæŒ‡å—: {enhanced_info['title']}")
                            self.logger.info(f"   ğŸ“ æ–‡ä»¶å: {enhanced_info['filename']}")
                            if version_info:
                                self.logger.info(f"   ğŸ·ï¸  ç‰ˆæœ¬: {version_info}")

                    break

            if not pdf_list_found:
                self.logger.warning(f"âš ï¸ Guidelineséƒ¨åˆ†ä¸‹æœªæ‰¾åˆ°pdfList")
                return []

            self.logger.info(f"âœ… Guidelinesæå–å®Œæˆï¼Œæ‰¾åˆ° {len(pdf_links)} ä¸ªæ ¸å¿ƒæŒ‡å—")
            return pdf_links

        except Exception as e:
            self.logger.error(f"âŒ Guidelinesæå–å¤±è´¥: {str(e)}")
            return []

    def _extract_version_info(self, link_element) -> str:
        """ä»é“¾æ¥å…ƒç´ ä¸­æå–ç‰ˆæœ¬ä¿¡æ¯"""
        try:
            # æŸ¥æ‰¾åŒä¸€æ®µè½ä¸­çš„ç‰ˆæœ¬ä¿¡æ¯
            parent_p = link_element.find_parent('p')
            if parent_p:
                # æŸ¥æ‰¾spanæ ‡ç­¾ä¸­çš„ç‰ˆæœ¬ä¿¡æ¯
                version_spans = parent_p.find_all('span')
                for span in version_spans:
                    span_text = span.get_text(strip=True)
                    if 'version' in span_text.lower() or 'Version' in span_text:
                        # æå–ç‰ˆæœ¬å·ï¼Œå¦‚"Version 1.2026" -> "1_2026"
                        version_clean = span_text.replace('Version', '').replace('version', '').strip()
                        version_clean = version_clean.replace('.', '_').replace(' ', '_')
                        return version_clean

            return ""
        except:
            return ""


    def _parse_patient_guidelines_bilingual(self, soup: BeautifulSoup, theme: ThemeConfig, language_filter: str = 'all') -> List[Dict[str, Any]]:
        """è§£æåŒè¯­æ‚£è€…æŒ‡å—é¡µé¢ - ä¼˜åŒ–è§£ææµç¨‹"""
        pdf_links = []
        self.logger.info(f"ğŸ” å¼€å§‹åŒè¯­æ‚£è€…æŒ‡å—è§£æ...")
        self.logger.info(f"ğŸŒ è¯­è¨€è¿‡æ»¤: {language_filter}")

        try:
            # ä¼˜åŒ–åˆ†æ”¯ï¼šæ ¹æ®è¯­è¨€è¿‡æ»¤é€‰æ‹©è§£æç­–ç•¥
            if language_filter == 'chinese':
                self.logger.info(f"ğŸ¯ é€‰æ‹©'ä»…ä¸­æ–‡ç‰ˆæœ¬'ï¼Œç›´æ¥è®¿é—®ç¿»è¯‘é¡µé¢ä¼˜åŒ–è§£æ...")
                return self._parse_translation_page_directly()
            elif language_filter == 'english':
                self.logger.info(f"ğŸ¯ é€‰æ‹©'ä»…è‹±æ–‡ç‰ˆæœ¬'ï¼Œè·³è¿‡ç¿»è¯‘é¡µé¢è§£æ...")
            else:
                self.logger.info(f"ğŸ¯ é€‰æ‹©'å…¨éƒ¨ç‰ˆæœ¬'ï¼Œæ‰§è¡Œå®Œæ•´ä¸‰æ­¥éª¤è§£æ...")

            # æ­¥éª¤1: ä»ä¸»é¡µé¢æå–è¯¦æƒ…é¡µé“¾æ¥
            self.logger.info(f"ğŸ“‹ æ­¥éª¤1: ä»ä¸»é¡µé¢æå–æ‚£è€…æŒ‡å—è¯¦æƒ…é¡µé“¾æ¥...")
            all_links = soup.find_all('a', href=True)
            detail_links = []

            for link in all_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)

                # æŸ¥æ‰¾è¯¦æƒ…é¡µé“¾æ¥æ ¼å¼: /guidelines-for-patients-details?patientGuidelineId=X
                if '/guidelines-for-patients-details?patientGuidelineId=' in href:
                    # æ­£ç¡®æ‹¼æ¥URL
                    if href.startswith('http'):
                        detail_url = href
                    else:
                        detail_url = 'https://www.nccn.org' + href

                    detail_links.append({
                        'url': detail_url,
                        'text': text
                    })

            self.logger.info(f"âœ… æ­¥éª¤1å®Œæˆï¼Œæ‰¾åˆ° {len(detail_links)} ä¸ªæ‚£è€…æŒ‡å—è¯¦æƒ…é¡µ")

            # æ­¥éª¤1.5: æŸ¥æ‰¾ç¿»è¯‘é¡µé¢é“¾æ¥ï¼ˆç”¨äºè·å–ä¸­æ–‡PDFï¼‰
            self.logger.info(f"ğŸ“‹ æ­¥éª¤1.5: æŸ¥æ‰¾ç¿»è¯‘é¡µé¢é“¾æ¥è·å–ä¸­æ–‡ç‰ˆæœ¬...")
            translation_links = []
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)

                # æŸ¥æ‰¾æŒ‡å‘ç¿»è¯‘é¡µé¢çš„é“¾æ¥
                if 'translations' in href.lower() or ('translations' in text.lower()):
                    # æ­£ç¡®æ‹¼æ¥URL
                    if href.startswith('http'):
                        translation_url = href
                    else:
                        translation_url = 'https://www.nccn.org' + href

                    translation_links.append({
                        'url': translation_url,
                        'text': text
                    })

            self.logger.info(f"âœ… æ‰¾åˆ° {len(translation_links)} ä¸ªç¿»è¯‘é¡µé¢é“¾æ¥")

            if not detail_links:
                self.logger.warning("æœªæ‰¾åˆ°æ‚£è€…æŒ‡å—è¯¦æƒ…é¡µé“¾æ¥")
                return []

            # æ­¥éª¤2: éå†è¯¦æƒ…é¡µæå–PDFé“¾æ¥
            self.logger.info(f"ğŸ“‹ æ­¥éª¤2: éå†è¯¦æƒ…é¡µæå–PDFé“¾æ¥...")
            max_pages = min(len(detail_links), 10)  # é™åˆ¶æœ€å¤§å¤„ç†é¡µé¢æ•°é¿å…è¶…æ—¶

            for i, detail in enumerate(detail_links[:max_pages]):
                try:
                    self.logger.info(f"ğŸ“„ [{i+1}/{max_pages}] å¤„ç†è¯¦æƒ…é¡µ: {detail['text']}")

                    response = self.session.get(detail['url'])
                    if response.status_code != 200:
                        self.logger.warning(f"æ— æ³•è®¿é—®è¯¦æƒ…é¡µ: {detail['url']}")
                        continue

                    detail_soup = BeautifulSoup(response.content, 'html.parser')
                    detail_links_page = detail_soup.find_all('a', href=True)

                    for link in detail_links_page:
                        href = link.get('href', '')
                        link_text = link.get_text(strip=True)

                        # æŸ¥æ‰¾PDFé“¾æ¥
                        if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                            # åº”ç”¨è¯­è¨€è¿‡æ»¤
                            if not self._should_include_pdf(href, language_filter, link_text):
                                continue

                            # æ­£ç¡®æ‹¼æ¥URL
                            if href.startswith('http'):
                                pdf_url = href
                            else:
                                pdf_url = 'https://www.nccn.org' + href

                            # ç¡®å®šæ ‡é¢˜
                            title = link_text if link_text else detail['text']

                            # ç¡®å®šç‰ˆæœ¬è¯­è¨€
                            version = self._detect_pdf_language(href, link_text)

                            pdf_info = {
                                'title': title,
                                'url': pdf_url,
                                'version': version
                            }

                            pdf_links.append(pdf_info)

                            self.logger.info(f"ğŸ“„ è¯¦æƒ…é¡µPDF: {title} ({version}) -> {pdf_url[:80]}...")

                except Exception as e:
                    self.logger.warning(f"å¤„ç†è¯¦æƒ…é¡µå¤±è´¥ {detail['text']}: {str(e)}")
                    continue

            # æ­¥éª¤3: è§£æç¿»è¯‘é¡µé¢è·å–ä¸­æ–‡PDF
            if translation_links and language_filter in ['all', 'chinese']:
                self.logger.info(f"ğŸ“‹ æ­¥éª¤3: è§£æç¿»è¯‘é¡µé¢è·å–ä¸­æ–‡PDF...")
                self.logger.info(f"ğŸŒ å°†è®¿é—® {len(translation_links)} ä¸ªç¿»è¯‘é¡µé¢")

                for i, translation in enumerate(translation_links):
                    try:
                        self.logger.info(f"ğŸŒ [{i+1}/{len(translation_links)}] è®¿é—®ç¿»è¯‘é¡µé¢: {translation['text']}")
                        self.logger.info(f"ğŸ”— URL: {translation['url']}")

                        response = self.session.get(translation['url'])
                        if response.status_code != 200:
                            self.logger.warning(f"æ— æ³•è®¿é—®ç¿»è¯‘é¡µé¢: {translation['url']}")
                            continue

                        translation_soup = BeautifulSoup(response.content, 'html.parser')

                        # ä½¿ç”¨è°ƒè¯•è„šæœ¬ä¸­éªŒè¯è¿‡çš„æ­£ç¡®æ–¹æ³•ï¼šä¸“é—¨æŸ¥æ‰¾Chinese Translationséƒ¨åˆ†
                        self.logger.info(f"ğŸ” æŸ¥æ‰¾Chinese Translationséƒ¨åˆ†...")
                        chinese_headers = translation_soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])

                        chinese_section = None
                        for header in chinese_headers:
                            if 'Chinese' in header.get_text():
                                chinese_section = header
                                self.logger.info(f"âœ… æ‰¾åˆ°Chinese Translationséƒ¨åˆ†: {header.get_text(strip=True)}")
                                break

                        translation_pdfs = 0

                        if chinese_section:
                            # ä»Chinese Translationséƒ¨åˆ†å¼€å§‹æŸ¥æ‰¾PDFé“¾æ¥
                            current = chinese_section
                            processed_sections = 0

                            # éå†Chinese Translationsåé¢çš„æ‰€æœ‰å…ƒç´ ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªè¯­è¨€æ ‡é¢˜
                            while current and processed_sections < 50:
                                current = current.find_next_sibling()

                                if current is None:
                                    break

                                if current.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:  # é‡åˆ°ä¸‹ä¸€ä¸ªè¯­è¨€éƒ¨åˆ†ï¼Œåœæ­¢
                                    break

                                # æŸ¥æ‰¾å½“å‰å…ƒç´ ä¸­çš„æ‰€æœ‰é“¾æ¥
                                links = current.find_all('a', href=True)

                                for link in links:
                                    href = link.get('href', '')
                                    link_text = link.get_text(strip=True)

                                    # æŸ¥æ‰¾PDFé“¾æ¥
                                    if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                                        # åº”ç”¨è¯­è¨€è¿‡æ»¤
                                        if not self._should_include_pdf(href, language_filter, link_text):
                                            continue

                                        # æ­£ç¡®æ‹¼æ¥URL
                                        if href.startswith('http'):
                                            pdf_url = href
                                        else:
                                            pdf_url = 'https://www.nccn.org' + href

                                        # ç¡®å®šæ ‡é¢˜
                                        title = link_text if link_text else 'Chinese Patient Guideline'
                                        if not title:
                                            filename = href.split('/')[-1].replace('.pdf', '')
                                            title = filename.replace('-zh', '').replace('-', ' ') + ' (Chinese)'

                                        # ä½¿ç”¨ä¿®å¤åçš„è¯­è¨€æ£€æµ‹
                                        detected_version = self._detect_pdf_language(href, link_text)
                                        if detected_version != 'Chinese':
                                            continue  # è·³è¿‡éä¸­æ–‡PDF

                                        pdf_info = {
                                            'title': title,
                                            'url': pdf_url,
                                            'version': 'Chinese'
                                        }

                                        # é¿å…é‡å¤æ·»åŠ 
                                        existing_urls = [p['url'] for p in pdf_links]
                                        if pdf_url not in existing_urls:
                                            pdf_links.append(pdf_info)
                                            translation_pdfs += 1

                                            self.logger.info(f"ğŸ‡¨ğŸ‡³ ç¿»è¯‘é¡µPDF: {title} -> {pdf_url[:80]}...")

                                processed_sections += 1

                            self.logger.info(f"âœ… ä»Chinese Translationséƒ¨åˆ†è§£æå®Œæˆï¼Œæ‰¾åˆ° {translation_pdfs} ä¸ªä¸­æ–‡PDF")
                        else:
                            # å¤‡ç”¨æ–¹æ³•ï¼šå¦‚æœæ‰¾ä¸åˆ°Chinese Translationséƒ¨åˆ†ï¼Œä½¿ç”¨å…¨é¡µé¢æœç´¢
                            self.logger.info(f"âš ï¸  æœªæ‰¾åˆ°Chinese Translationséƒ¨åˆ†ï¼Œä½¿ç”¨å¤‡ç”¨æœç´¢æ–¹æ³•...")
                            translation_links_page = translation_soup.find_all('a', href=True)

                            for link in translation_links_page:
                                href = link.get('href', '')
                                link_text = link.get_text(strip=True)

                                # æŸ¥æ‰¾PDFé“¾æ¥
                                if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                                    # åº”ç”¨è¯­è¨€è¿‡æ»¤
                                    if not self._should_include_pdf(href, language_filter, link_text):
                                        continue

                                    # æ­£ç¡®æ‹¼æ¥URL
                                    if href.startswith('http'):
                                        pdf_url = href
                                    else:
                                        pdf_url = 'https://www.nccn.org' + href

                                    # ç¡®å®šæ ‡é¢˜
                                    title = link_text if link_text else 'Chinese Patient Guideline'
                                    if not title:
                                        filename = href.split('/')[-1].replace('.pdf', '')
                                        title = filename.replace('-zh', '').replace('-', ' ') + ' (Chinese)'

                                    # ä½¿ç”¨ä¿®å¤åçš„è¯­è¨€æ£€æµ‹
                                    detected_version = self._detect_pdf_language(href, link_text)
                                    if detected_version != 'Chinese':
                                        continue  # è·³è¿‡éä¸­æ–‡PDF

                                    pdf_info = {
                                        'title': title,
                                        'url': pdf_url,
                                        'version': 'Chinese'
                                    }

                                    # é¿å…é‡å¤æ·»åŠ 
                                    existing_urls = [p['url'] for p in pdf_links]
                                    if pdf_url not in existing_urls:
                                        pdf_links.append(pdf_info)
                                        translation_pdfs += 1

                                        self.logger.info(f"ğŸ‡¨ğŸ‡³ ç¿»è¯‘é¡µPDF: {title} -> {pdf_url[:80]}...")

                        self.logger.info(f"âœ… ç¿»è¯‘é¡µé¢ {translation['text']} æ‰¾åˆ° {translation_pdfs} ä¸ªä¸­æ–‡PDF")

                    except Exception as e:
                        self.logger.warning(f"å¤„ç†ç¿»è¯‘é¡µé¢å¤±è´¥ {translation['text']}: {str(e)}")
                        continue

            # æœ€ç»ˆç»Ÿè®¡
            chinese_count = sum(1 for p in pdf_links if p['version'] == 'Chinese')
            english_count = len(pdf_links) - chinese_count
            spanish_count = sum(1 for p in pdf_links if p['version'] == 'Spanish')

            self.logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
            self.logger.info(f"   æ€»PDFæ•°: {len(pdf_links)}")
            self.logger.info(f"   ä¸­æ–‡ç‰ˆæœ¬: {chinese_count}")
            self.logger.info(f"   è‹±æ–‡ç‰ˆæœ¬: {english_count}")
            self.logger.info(f"   è¥¿ç­ç‰™è¯­ç‰ˆæœ¬: {spanish_count}")

            self.logger.info(f"âœ… ä¸‰æ­¥éª¤è§£æå®Œæˆï¼Œæ€»å…±æ‰¾åˆ° {len(pdf_links)} ä¸ªPDFæ–‡ä»¶")
            return pdf_links

        except Exception as e:
            self.logger.error(f"âŒ åŒè¯­æ‚£è€…æŒ‡å—è§£æå¤±è´¥: {str(e)}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []

    def _extract_pdfs_from_main_page(self, soup: BeautifulSoup, language_filter: str = 'all') -> List[Dict[str, Any]]:
        """ä»ä¸»é¡µé¢ç›´æ¥æå–PDFé“¾æ¥çš„å¤‡ç”¨æ–¹æ³•"""
        pdf_links = []

        self.logger.info(f"ğŸ” ä»ä¸»é¡µé¢ç›´æ¥æå–PDFé“¾æ¥...")
        all_links = soup.find_all('a', href=True)

        for link in all_links:
            href = link.get('href', '')

            # æŸ¥æ‰¾æ‚£è€…æŒ‡å—PDFé“¾æ¥
            if href.endswith('.pdf') and '/patients/guidelines/content/PDF/' in href:
                # åº”ç”¨è¯­è¨€è¿‡æ»¤
                if not self._should_include_pdf(href, language_filter):
                    continue

                # æ­£ç¡®æ‹¼æ¥URL
                if href.startswith('http'):
                    pdf_url = href
                else:
                    base_url = 'https://www.nccn.org'
                    if href.startswith('/'):
                        pdf_url = base_url + href
                    else:
                        pdf_url = urljoin(base_url, href)

                title = link.text.strip()
                if not title:
                    filename = href.split('/')[-1].replace('.pdf', '')
                    if filename.endswith('-zh'):
                        title = filename[:-3].replace('-', ' ') + ' (Chinese)'
                    else:
                        title = filename.replace('-', ' ')

                # ç¡®å®šç‰ˆæœ¬è¯­è¨€
                version = 'Chinese' if '-zh' in href.lower() or 'chinese' in href.lower() else 'English'

                pdf_info = {
                    'title': title,
                    'url': pdf_url,
                    'version': version
                }

                pdf_links.append(pdf_info)

                if len(pdf_links) <= 5:
                    self.logger.info(f"ğŸ“„ ç›´æ¥æ‰¾åˆ°PDF: {title} ({version}) -> {pdf_url[:60]}...")

        self.logger.info(f"âœ… ä¸»é¡µé¢ç›´æ¥è§£æå®Œæˆï¼Œæ‰¾åˆ° {len(pdf_links)} ä¸ªPDFæ–‡ä»¶")
        return pdf_links

    def _extract_pdfs_from_patient_detail_page_simple(self, soup: BeautifulSoup, guideline_title: str, language_filter: str = 'all') -> List[Dict[str, Any]]:
        """ä»æ‚£è€…æŒ‡å—è¯¦æƒ…é¡µæå–PDFé“¾æ¥ - ç®€åŒ–ç‰ˆæœ¬"""
        pdf_links = []

        try:
            # æŸ¥æ‰¾æ‚£è€…æŒ‡å—PDFé“¾æ¥
            all_links = soup.find_all('a', href=True)

            for link in all_links:
                href = link.get('href', '')

                # æŸ¥æ‰¾æ‚£è€…æŒ‡å—PDFé“¾æ¥
                if href.endswith('.pdf') and '/patients/guidelines/content/PDF/' in href:
                    # åº”ç”¨è¯­è¨€è¿‡æ»¤
                    if not self._should_include_pdf(href, language_filter):
                        continue

                    # æ­£ç¡®æ‹¼æ¥URL
                    if href.startswith('http'):
                        pdf_url = href
                    else:
                        base_url = 'https://www.nccn.org'
                        if href.startswith('/'):
                            pdf_url = base_url + href
                        else:
                            pdf_url = urljoin(base_url, href)

                    title = link.text.strip()
                    if not title:
                        filename = href.split('/')[-1].replace('.pdf', '')
                        if filename.endswith('-zh'):
                            title = filename[:-3].replace('-', ' ') + ' (Chinese)'
                        else:
                            title = filename.replace('-', ' ')

                    # ç¡®å®šç‰ˆæœ¬è¯­è¨€
                    version = 'Chinese' if '-zh' in href.lower() or 'chinese' in href.lower() else 'English'

                    pdf_info = {
                        'title': title,
                        'url': pdf_url,
                        'version': version
                    }

                    pdf_links.append(pdf_info)

                    if len(pdf_links) <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        self.logger.info(f"ğŸ“„ è¯¦æƒ…é¡µPDF: {title} ({version}) -> {pdf_url[:60]}...")

            self.logger.info(f"ğŸ“Š è¯¦æƒ…é¡µè§£æ: æ‰¾åˆ° {len(pdf_links)} ä¸ªPDFæ–‡ä»¶")
            return pdf_links

        except Exception as e:
            self.logger.error(f"âŒ ä»æ‚£è€…æŒ‡å—è¯¦æƒ…é¡µæå–PDFå¤±è´¥: {str(e)}")
            return []

    def _parse_translation_page_directly(self) -> List[Dict[str, Any]]:
        """ç›´æ¥è§£æç¿»è¯‘é¡µé¢è·å–ä¸­æ–‡PDF - ä¼˜åŒ–ç‰ˆæœ¬"""
        pdf_links = []

        try:
            # ç›´æ¥è®¿é—®ç¿»è¯‘é¡µé¢
            translation_url = "https://www.nccn.org/global/what-we-do/guidelines-for-patients-translations"
            self.logger.info(f"ğŸŒ ç›´æ¥è®¿é—®ç¿»è¯‘é¡µé¢: {translation_url}")

            response = self.session.get(translation_url)
            if response.status_code != 200:
                self.logger.warning(f"æ— æ³•è®¿é—®ç¿»è¯‘é¡µé¢: {translation_url}")
                return []

            translation_soup = BeautifulSoup(response.content, 'html.parser')

            # æŸ¥æ‰¾Chinese Translationséƒ¨åˆ†
            self.logger.info(f"ğŸ” æŸ¥æ‰¾Chinese Translationséƒ¨åˆ†...")
            chinese_headers = translation_soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])

            chinese_section = None
            for header in chinese_headers:
                if 'Chinese' in header.get_text():
                    chinese_section = header
                    self.logger.info(f"âœ… æ‰¾åˆ°Chinese Translationséƒ¨åˆ†: {header.get_text(strip=True)}")
                    break

            if not chinese_section:
                self.logger.warning("æœªæ‰¾åˆ°Chinese Translationséƒ¨åˆ†")
                return []

            # ä»Chinese Translationséƒ¨åˆ†å¼€å§‹æŸ¥æ‰¾PDFé“¾æ¥
            current = chinese_section
            processed_sections = 0

            # éå†Chinese Translationsåé¢çš„æ‰€æœ‰å…ƒç´ ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªè¯­è¨€æ ‡é¢˜
            while current and processed_sections < 50:
                current = current.find_next_sibling()

                if current is None:
                    break

                if current.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:  # é‡åˆ°ä¸‹ä¸€ä¸ªè¯­è¨€éƒ¨åˆ†ï¼Œåœæ­¢
                    self.logger.info(f"ğŸ›‘ é‡åˆ°ä¸‹ä¸€ä¸ªè¯­è¨€éƒ¨åˆ†: {current.get_text(strip=True)[:50]}...")
                    break

                # æŸ¥æ‰¾å½“å‰å…ƒç´ ä¸­çš„æ‰€æœ‰é“¾æ¥
                links = current.find_all('a', href=True)

                for link in links:
                    href = link.get('href', '')
                    link_text = link.get_text(strip=True)

                    # æŸ¥æ‰¾PDFé“¾æ¥
                    if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                        # æ­£ç¡®æ‹¼æ¥URL
                        if href.startswith('http'):
                            pdf_url = href
                        else:
                            pdf_url = 'https://www.nccn.org' + href

                        # ç¡®å®šæ ‡é¢˜
                        title = link_text if link_text else 'Chinese Patient Guideline'
                        if not title:
                            filename = href.split('/')[-1].replace('.pdf', '')
                            title = filename.replace('-zh', '').replace('-', ' ') + ' (Chinese)'

                        pdf_info = {
                            'title': title,
                            'url': pdf_url,
                            'version': 'Chinese'
                        }

                        # é¿å…é‡å¤æ·»åŠ 
                        existing_urls = [p['url'] for p in pdf_links]
                        if pdf_url not in existing_urls:
                            pdf_links.append(pdf_info)
                            self.logger.info(f"ğŸ‡¨ğŸ‡³ ç¿»è¯‘é¡µPDF: {title} -> {pdf_url[:80]}...")

                processed_sections += 1

            self.logger.info(f"âœ… ç›´æ¥è§£æç¿»è¯‘é¡µé¢å®Œæˆï¼Œæ‰¾åˆ° {len(pdf_links)} ä¸ªä¸­æ–‡PDF")
            return pdf_links

        except Exception as e:
            self.logger.error(f"âŒ ç›´æ¥è§£æç¿»è¯‘é¡µé¢å¤±è´¥: {str(e)}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []

    def _parse_patient_guidelines_chinese(self, soup: BeautifulSoup, theme: ThemeConfig) -> List[Dict[str, Any]]:
        """è§£ææ‚£è€…æŒ‡å—ä¸­æ–‡ç‰ˆæœ¬ - ç›´æ¥è®¿é—®ç¿»è¯‘é¡µé¢"""
        try:
            # ç›´æ¥è°ƒç”¨ç°æœ‰çš„ç¿»è¯‘é¡µé¢è§£ææ–¹æ³•
            self.logger.info(f"ğŸ¯ æ‚£è€…æŒ‡å—ä¸­æ–‡ç‰ˆæœ¬ï¼šç›´æ¥è®¿é—®ç¿»è¯‘é¡µé¢è·å–ä¸­æ–‡PDF")
            pdf_links = self._parse_translation_page_directly()

            self.logger.info(f"âœ… æ‚£è€…æŒ‡å—ä¸­æ–‡ç‰ˆæœ¬è§£æå®Œæˆï¼Œæ€»å…±æ‰¾åˆ° {len(pdf_links)} ä¸ªä¸­æ–‡PDF")
            return pdf_links

        except Exception as e:
            self.logger.error(f"âŒ è§£ææ‚£è€…æŒ‡å—ä¸­æ–‡ç‰ˆæœ¬å¤±è´¥: {str(e)}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []

    def _parse_patient_guidelines_english(self, soup: BeautifulSoup, theme: ThemeConfig, language_filter: str = 'all') -> List[Dict[str, Any]]:
        """è§£ææ‚£è€…æŒ‡å—è‹±æ–‡ç‰ˆæœ¬ - åŒæ­¥éª¤è§£æï¼Œåªæå–è‹±æ–‡PDF"""
        try:
            self.logger.info(f"ğŸ¯ æ‚£è€…æŒ‡å—è‹±æ–‡ç‰ˆæœ¬ï¼šå¼€å§‹åŒæ­¥éª¤è§£ææµç¨‹")

            # æ­¥éª¤1: è·å–æ‰€æœ‰guidelines-detailé“¾æ¥
            self.logger.info(f"ğŸ” æ­¥éª¤1: è·å–guidelines-detailé“¾æ¥...")
            sub_links = self._get_sub_links_patient_guidelines(soup, theme.url)

            if not sub_links:
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ‚£è€…æŒ‡å—è¯¦æƒ…é¡µé“¾æ¥")
                return []

            self.logger.info(f"ğŸ“Š æ‰¾åˆ° {len(sub_links)} ä¸ªæ‚£è€…æŒ‡å—è¯¦æƒ…é¡µ")

            # æ­¥éª¤2: éå†æ¯ä¸ªè¯¦æƒ…é¡µï¼Œåªæå–è‹±æ–‡PDF
            pdf_links = []
            for i, sub_url in enumerate(sub_links, 1):
                self.logger.info(f"ğŸ“„ [{i}/{len(sub_links)}] å¤„ç†è¯¦æƒ…é¡µ: {sub_url.split('?')[0].split('/')[-1]}")

                try:
                    response = self.session.get(sub_url)
                    if response.status_code != 200:
                        self.logger.warning(f"æ— æ³•è®¿é—®è¯¦æƒ…é¡µ: {sub_url}")
                        continue

                    sub_soup = BeautifulSoup(response.content, 'html.parser')

                    # æŸ¥æ‰¾PDFé“¾æ¥
                    for link in sub_soup.find_all('a', href=True):
                        href = link.get('href', '')
                        link_text = link.get_text(strip=True)

                        # åªå¤„ç†PDFé“¾æ¥
                        if '/patients/guidelines/content/PDF/' in href and href.endswith('.pdf'):
                            # æ­£ç¡®æ‹¼æ¥URL
                            if href.startswith('http'):
                                pdf_url = href
                            else:
                                pdf_url = 'https://www.nccn.org' + href

                            # æ£€æµ‹è¯­è¨€ï¼Œåªä¿ç•™è‹±æ–‡ç‰ˆæœ¬
                            detected_language = self._detect_pdf_language(pdf_url, link_text)

                            if detected_language in ['English', 'Unknown']:
                                # ç¡®å®šæ ‡é¢˜
                                title = link_text if link_text else 'Patient Guideline'
                                if not title or title == 'Patient Guideline':
                                    filename = href.split('/')[-1].replace('.pdf', '')
                                    title = filename.replace('-patient', '').replace('-', ' ').title() + ' (English)'

                                # é¿å…é‡å¤æ·»åŠ 
                                existing_urls = [p['url'] for p in pdf_links]
                                if pdf_url not in existing_urls:
                                    pdf_info = {
                                        'title': title,
                                        'url': pdf_url,
                                        'version': detected_language,
                                        'source_page': sub_url
                                    }
                                    pdf_links.append(pdf_info)
                                    self.logger.info(f"ğŸ“„ è‹±æ–‡PDF: {title} -> {pdf_url[:80]}...")

                    # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                    time.sleep(random.uniform(1, 3))

                except Exception as e:
                    self.logger.error(f"å¤„ç†è¯¦æƒ…é¡µå¤±è´¥ {sub_url}: {str(e)}")
                    continue

            self.logger.info(f"âœ… æ‚£è€…æŒ‡å—è‹±æ–‡ç‰ˆæœ¬è§£æå®Œæˆï¼Œæ€»å…±æ‰¾åˆ° {len(pdf_links)} ä¸ªè‹±æ–‡PDF")
            return pdf_links

        except Exception as e:
            self.logger.error(f"âŒ è§£ææ‚£è€…æŒ‡å—è‹±æ–‡ç‰ˆæœ¬å¤±è´¥: {str(e)}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []

    def _detect_pdf_language(self, pdf_url: str, link_text: str = "") -> str:
        """æ£€æµ‹PDFçš„è¯­è¨€ç‰ˆæœ¬"""
        url_lower = pdf_url.lower()
        text_lower = link_text.lower()

        # æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«ä¸­æ–‡æ ‡è¯†ï¼ˆä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…ï¼Œé¿å…è¯¯åˆ¤ï¼‰
        if any(indicator in url_lower for indicator in ['-zh', '-chinese']):
            return 'Chinese'
        elif any(indicator in url_lower for indicator in ['-chi']) and 'children' not in url_lower:
            return 'Chinese'
        elif any(indicator in url_lower for indicator in ['-ch(', '-ch)']):
            return 'Chinese'
        elif 'chinese' in text_lower:
            return 'Chinese'
        # æ£€æŸ¥è¥¿ç­ç‰™è¯­æ ‡è¯†ï¼ˆä¼˜å…ˆæ£€æŸ¥ï¼Œé¿å…ä¸­æ–‡è¯¯åˆ¤ï¼‰
        elif any(indicator in url_lower for indicator in ['-es', '-esl', '-es_', '-spanish']):
            return 'Spanish'
        elif 'spanish' in text_lower:
            return 'Spanish'
        # æ£€æŸ¥å…¶ä»–è¯­è¨€æ ‡è¯†
        elif any(indicator in url_lower for indicator in ['-ar', '-arabic', 'arabic']):
            return 'Arabic'
        elif any(indicator in url_lower for indicator in ['-fr', '-french', 'french']):
            return 'French'
        elif any(indicator in url_lower for indicator in ['-hi', '-hindi', 'hindi']):
            return 'Hindi'
        elif any(indicator in url_lower for indicator in ['-jp', '-japanese', 'japanese']):
            return 'Japanese'
        elif any(indicator in url_lower for indicator in ['-kr', '-korean', 'korean']):
            return 'Korean'
        elif any(indicator in url_lower for indicator in ['-po', '-polish', 'polish']):
            return 'Polish'
        elif any(indicator in url_lower for indicator in ['-pt', '-portuguese', 'portuguese']):
            return 'Portuguese'
        elif any(indicator in url_lower for indicator in ['-ru', '-russian', 'russian']):
            return 'Russian'
        elif any(indicator in url_lower for indicator in ['-vi', '-vietnamese', 'vietnamese']):
            return 'Vietnamese'
        else:
            return 'English'  # é»˜è®¤è®¤ä¸ºæ˜¯è‹±æ–‡

    def _should_include_pdf(self, pdf_url: str, language_filter: str, link_text: str = "") -> bool:
        """æ ¹æ®è¯­è¨€è¿‡æ»¤è§„åˆ™å’Œå†…å®¹ç±»å‹åˆ¤æ–­æ˜¯å¦åŒ…å«è¯¥PDF

        Args:
            pdf_url: PDF URL
            language_filter: è¯­è¨€è¿‡æ»¤é€‰é¡¹ ('all', 'english', 'chinese')
            link_text: é“¾æ¥æ–‡æœ¬ï¼ˆç”¨äºè¯­è¨€æ£€æµ‹ï¼‰

        Returns:
            bool: æ˜¯å¦åº”è¯¥åŒ…å«è¯¥PDF
        """
        # é¦–å…ˆæ£€æŸ¥é“¾æ¥æ–‡æœ¬ï¼Œè¯†åˆ«æ–‡ä»¶ç±»å‹
        lower_text = link_text.lower()

        # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ–‡ä»¶ç±»å‹
        exclude_patterns = [
            'framework',  # Basic Framework, Core Frameworkç­‰
            'exhibitor',  # ä¼šè®®å‚å±•å•†æ‰‹å†Œ
            'conference', # ä¼šè®®ç›¸å…³
            'prospectus', # æ‹›è‚¡è¯´æ˜ä¹¦
            'user guide', # ç”¨æˆ·æŒ‡å—
            'order template', # è®¢å•æ¨¡æ¿
            'middle east', # ä¸­ä¸œåœ°åŒº
            'north africa', # åŒ—é
            'sub-saharan africa', # æ’’å“ˆæ‹‰ä»¥å—éæ´²
            'mena', # ä¸­ä¸œåŒ—éåœ°åŒº
            'arabic', # é˜¿æ‹‰ä¼¯è¯­
            'hindi', # å°åœ°è¯­
            'portuguese', # è‘¡è„ç‰™è¯­
            'spanish', # è¥¿ç­ç‰™è¯­ - åœ¨ä»»ä½•æ¨¡å¼ä¸‹éƒ½è¦è¿‡æ»¤
        ]

        for pattern in exclude_patterns:
            if pattern in lower_text:
                self.logger.debug(f"è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ–‡ä»¶ç±»å‹: {link_text[:50]} (åŒ…å«: {pattern})")
                return False

        # è¯­è¨€è¿‡æ»¤é€»è¾‘
        if language_filter == 'all':
            # åœ¨å…¨éƒ¨æ¨¡å¼ä¸‹ï¼Œåªä¿ç•™è‹±æ–‡å’Œä¸­æ–‡ç‰ˆæœ¬
            detected_language = self._detect_pdf_language(pdf_url, link_text)
            return detected_language in ['English', 'Chinese']

        # æ£€æµ‹PDFçš„å®é™…è¯­è¨€
        detected_language = self._detect_pdf_language(pdf_url, link_text)

        if language_filter == 'chinese':
            return detected_language == 'Chinese'
        elif language_filter == 'english':
            # åªä¿ç•™è‹±æ–‡ç‰ˆæœ¬ï¼Œå¹¶ä¸”å¿…é¡»æ˜¯æ ¸å¿ƒæŒ‡å—æ–‡ä»¶
            if detected_language == 'English':
                # è¿›ä¸€æ­¥è¿‡æ»¤ï¼šç¡®ä¿æ˜¯NCCN Guidelinesæˆ–è€…æ˜¯ç™Œç—‡ç›¸å…³çš„è‹±æ–‡æŒ‡å—
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ ¸å¿ƒæŒ‡å—ï¼ˆNCCN Guidelinesæˆ–è€…æ˜¯ç™Œç—‡ç—‡çŠ¶ç›¸å…³çš„æŒ‡å—ï¼‰
                is_guideline = (
                    'guidelines' in lower_text or
                    'nausea and vomiting' in lower_text or
                    'blood clots' in lower_text or
                    'fatigue' in lower_text or
                    'distress' in lower_text or
                    'pain' in lower_text or
                    'anemia' in lower_text or
                    'neutropenia' in lower_text or
                    'immunotherapy' in lower_text or
                    'palliative care' in lower_text
                )

                if is_guideline:
                    return True
                else:
                    self.logger.debug(f"è¿‡æ»¤æ‰éæ ¸å¿ƒè‹±æ–‡æ–‡ä»¶: {link_text[:50]}")
                    return False
            else:
                return False
        else:
            return True

    def _get_pdfs_from_detail_page(self, detail_url: str, guideline_title: str, language_filter: str = 'all', theme=None) -> List[Dict[str, Any]]:
        """ä»æŒ‡å—è¯¦æƒ…é¡µé¢è·å–PDFé“¾æ¥

        Args:
            detail_url: è¯¦æƒ…é¡µé¢URL
            guideline_title: æŒ‡å—æ ‡é¢˜
            language_filter: è¯­è¨€è¿‡æ»¤é€‰é¡¹ ('all', 'english', 'chinese')
            theme: ä¸»é¢˜é…ç½®ï¼ˆå¯é€‰ï¼Œç”¨äºæ§åˆ¶æå–æ–¹å¼ï¼‰

        Returns:
            List[Dict]: PDFä¿¡æ¯åˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ” è¯·æ±‚è¯¦æƒ…é¡µé¢: {detail_url}")
            response = self.session.get(detail_url, timeout=30)
            response.raise_for_status()

            self.logger.info(f"âœ… è¯¦æƒ…é¡µé¢è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            soup = BeautifulSoup(response.content, 'html.parser')

            pdf_links = []
            self.logger.info(f"ğŸ” å¼€å§‹è§£æè¯¦æƒ…é¡µé¢PDFé“¾æ¥...")

            # æ ¹æ®ä¸»é¢˜ç±»å‹é€‰æ‹©æå–æ–¹æ³•
            if theme and getattr(theme, 'guidelines_only', False):
                self.logger.info(f"ğŸ¯ ä½¿ç”¨Guidelines-onlyæå–æ¨¡å¼")
                pdf_links = self._extract_guidelines_only(soup, language_filter)
                return pdf_links

            # æ–¹æ³•1: æŸ¥æ‰¾æ‰€æœ‰ç›´æ¥PDFé“¾æ¥
            all_links = soup.find_all('a', href=True)
            pdf_direct_links = []

            for link in all_links:
                href = link.get('href', '')
                if href.endswith('.pdf'):
                    # åº”ç”¨è¯­è¨€è¿‡æ»¤
                    link_text = link.text.strip()
                    if not self._should_include_pdf(href, language_filter, link_text):
                        continue

                    # æ­£ç¡®æ‹¼æ¥URL - ä½¿ç”¨NCCNæ ¹åŸŸå
                    if href.startswith('http'):
                        pdf_url = href
                    else:
                        # ä¿®å¤URLæ‹¼æ¥é—®é¢˜ï¼šä½¿ç”¨NCCNæ ¹åŸŸåä½œä¸ºåŸºç¡€
                        base_url = 'https://www.nccn.org'
                        if href.startswith('/'):
                            pdf_url = base_url + href
                        else:
                            pdf_url = urljoin(base_url, href)

                    title = link_text if link_text else guideline_title

                    # ç¡®å®šç‰ˆæœ¬è¯­è¨€
                    version = self._detect_pdf_language(href, link_text)

                    pdf_direct_links.append({
                        'title': title,
                        'url': pdf_url,
                        'version': version
                    })

                    self.logger.info(f"ğŸ“„ æ‰¾åˆ°ç›´æ¥PDFé“¾æ¥: {title[:50]} ({version})...")
                    self.logger.debug(f"ğŸ”— PDF URL: {pdf_url}")

            self.logger.info(f"ğŸ“Š æ–¹æ³•1æ‰¾åˆ° {len(pdf_direct_links)} ä¸ªç›´æ¥PDFé“¾æ¥")

            # æ–¹æ³•2: æŸ¥æ‰¾pdfListç±»çš„é“¾æ¥
            pdf_lists = soup.find_all('ul', class_='pdfList')
            pdf_list_links = []

            for pdf_list in pdf_lists:
                self.logger.info(f"ğŸ“‹ æ‰¾åˆ°pdfListåŒºåŸŸï¼ŒåŒ…å« {len(pdf_list.find_all('a', href=True))} ä¸ªé“¾æ¥")

                for link in pdf_list.find_all('a', href=True):
                    href = link.get('href', '')
                    if href.endswith('.pdf'):
                        # åº”ç”¨è¯­è¨€è¿‡æ»¤
                        link_text = link.text.strip()
                        if not self._should_include_pdf(href, language_filter, link_text):
                            continue

                        # æ­£ç¡®æ‹¼æ¥URL - ä½¿ç”¨NCCNæ ¹åŸŸå
                        if href.startswith('http'):
                            pdf_url = href
                        else:
                            # ä¿®å¤URLæ‹¼æ¥é—®é¢˜ï¼šä½¿ç”¨NCCNæ ¹åŸŸåä½œä¸ºåŸºç¡€
                            base_url = 'https://www.nccn.org'
                            if href.startswith('/'):
                                pdf_url = base_url + href
                            else:
                                pdf_url = urljoin(base_url, href)

                        title = link_text if link_text else guideline_title

                        # ç¡®å®šç‰ˆæœ¬è¯­è¨€
                        version = self._detect_pdf_language(href, link_text)

                        pdf_list_links.append({
                            'title': title,
                            'url': pdf_url,
                            'version': version
                        })

                        self.logger.info(f"ğŸ“„ æ‰¾åˆ°pdfList PDFé“¾æ¥: {title[:50]} ({version})...")
                        self.logger.debug(f"ğŸ”— PDF URL: {pdf_url}")

            self.logger.info(f"ğŸ“Š æ–¹æ³•2æ‰¾åˆ° {len(pdf_list_links)} ä¸ªpdfList PDFé“¾æ¥")

            # åˆå¹¶ç»“æœï¼Œå»é‡
            pdf_links = pdf_direct_links.copy()

            for link_info in pdf_list_links:
                # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ç›¸åŒURL
                if not any(p['url'] == link_info['url'] for p in pdf_links):
                    pdf_links.append(link_info)

            self.logger.info(f"ğŸ¯ å»é‡åæ€»è®¡: {len(pdf_links)} ä¸ªPDFé“¾æ¥")

            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°PDFï¼Œè¾“å‡ºè°ƒè¯•ä¿¡æ¯
            if not pdf_links:
                self.logger.warning(f"âš ï¸ è¯¦æƒ…é¡µé¢æœªæ‰¾åˆ°PDFé“¾æ¥ï¼Œå¼€å§‹è°ƒè¯•...")
                self.logger.debug(f"é¡µé¢æ ‡é¢˜: {soup.title.text if soup.title else 'N/A'}")
                self.logger.debug(f"æ€»é“¾æ¥æ•°: {len(all_links)}")

                # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«pdfçš„é“¾æ¥ï¼ˆåŒ…æ‹¬ä¸å®Œæ•´çš„ï¼‰
                pdf_mentioned_links = [link for link in all_links if 'pdf' in link.get('href', '').lower()]
                self.logger.debug(f"åŒ…å«'pdf'çš„é“¾æ¥æ•°: {len(pdf_mentioned_links)}")

                for i, link in enumerate(pdf_mentioned_links[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                    self.logger.debug(f"  é“¾æ¥{i+1}: {link.get('href', 'N/A')}")

            return pdf_links

        except Exception as e:
            self.logger.error(f"âŒ ä»è¯¦æƒ…é¡µé¢è·å–PDFå¤±è´¥ {detail_url}: {str(e)}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []

    def _extract_guidelines_only(self, soup: BeautifulSoup, language_filter: str = 'all') -> List[Dict[str, Any]]:
        """ä¸“é—¨æå–"Guidelines"éƒ¨åˆ†çš„æ ¸å¿ƒæŒ‡å—PDFï¼Œå¿½ç•¥å…¶ä»–é™„åŠ æ–‡ä»¶

        Args:
            soup: BeautifulSoupå¯¹è±¡
            language_filter: è¯­è¨€è¿‡æ»¤é€‰é¡¹ ('all', 'english', 'chinese')

        Returns:
            List[Dict]: åªåŒ…å«æ ¸å¿ƒæŒ‡å—çš„PDFä¿¡æ¯åˆ—è¡¨
        """
        try:
            self.logger.info(f"ğŸ” ä¸“é—¨æå–Guidelineséƒ¨åˆ†çš„æ ¸å¿ƒæŒ‡å—...")

            # æŸ¥æ‰¾Guidelinesæ ‡é¢˜
            guidelines_headers = soup.find_all('h4', class_='GL', string='Guidelines')

            if not guidelines_headers:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ°Guidelineséƒ¨åˆ†ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                return self._get_pdfs_from_detail_page_fallback(soup, language_filter)

            pdf_links = []

            for header in guidelines_headers:
                self.logger.info(f"âœ… æ‰¾åˆ°Guidelineséƒ¨åˆ†")

                # æŸ¥æ‰¾è¯¥headerä¸‹çš„æ‰€æœ‰ç›¸é‚»çš„pdfListå…ƒç´ 
                current_element = header.next_sibling

                while current_element:
                    if hasattr(current_element, 'name') and current_element.name == 'ul':
                        if 'pdfList' in current_element.get('class', []):
                            # æå–è¿™ä¸ªpdfListä¸­çš„æ‰€æœ‰PDF
                            pdf_list_links = self._extract_pdfs_from_list(current_element, language_filter)
                            if pdf_list_links:
                                pdf_links.extend(pdf_list_links)
                                self.logger.info(f"ğŸ“‹ Guidelineséƒ¨åˆ†æ‰¾åˆ° {len(pdf_list_links)} ä¸ªæ ¸å¿ƒPDF")

                    # å¦‚æœé‡åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ï¼Œåœæ­¢æŸ¥æ‰¾
                    if hasattr(current_element, 'name') and current_element.name == 'h4':
                        break

                    current_element = current_element.next_sibling

                    # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ— é™å¾ªç¯
                    if current_element is None:
                        break

            # å»é‡
            unique_pdfs = []
            seen_urls = set()
            for pdf in pdf_links:
                if pdf['url'] not in seen_urls:
                    seen_urls.add(pdf['url'])
                    unique_pdfs.append(pdf)

            self.logger.info(f"ğŸ¯ Guidelineséƒ¨åˆ†æå–ç»“æœ: {len(unique_pdfs)} ä¸ªæ ¸å¿ƒPDF")
            return unique_pdfs

        except Exception as e:
            self.logger.error(f"âŒ æå–Guidelineséƒ¨åˆ†PDFå¤±è´¥: {str(e)}")
            return self._get_pdfs_from_detail_page_fallback(soup, language_filter)

    def _extract_pdfs_from_list(self, pdf_list_element, language_filter: str = 'all') -> List[Dict[str, Any]]:
        """ä»pdfListå…ƒç´ ä¸­æå–PDFé“¾æ¥

        Args:
            pdf_list_element: pdfListçš„BeautifulSoupå…ƒç´ 
            language_filter: è¯­è¨€è¿‡æ»¤é€‰é¡¹

        Returns:
            List[Dict]: PDFä¿¡æ¯åˆ—è¡¨
        """
        pdf_links = []

        try:
            for link in pdf_list_element.find_all('a', href=True):
                href = link.get('href', '')
                if href.endswith('.pdf'):
                    # åº”ç”¨è¯­è¨€è¿‡æ»¤
                    link_text = link.text.strip()
                    if not self._should_include_pdf(href, language_filter, link_text):
                        continue

                    # æ„å»ºå®Œæ•´çš„PDF URL
                    if href.startswith('http'):
                        pdf_url = href
                    else:
                        base_url = 'https://www.nccn.org'
                        if href.startswith('/'):
                            pdf_url = base_url + href
                        else:
                            pdf_url = urljoin(base_url, href)

                    # æå–ç‰ˆæœ¬ä¿¡æ¯
                    version_info = self._extract_version_info(link)

                    # æå–æ ‡é¢˜å’Œæ–‡ä»¶å
                    title = link_text if link_text else "NCCN_Guideline"

                    # å¢å¼ºæ ‡é¢˜ä¿¡æ¯ï¼ˆä¼ é€’PDF URLç”¨äºæå–åŸå§‹æ–‡ä»¶åï¼‰
                    enhanced_info = self._enhance_pdf_info(title, version_info, pdf_url)

                    # ç¡®å®šè¯­è¨€ç‰ˆæœ¬ - ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„è¯­è¨€å‚æ•°
                    if language_filter == 'chinese':
                        language = 'Chinese'
                    elif language_filter == 'english':
                        language = 'English'
                    else:
                        # é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
                        language = self._detect_pdf_language(href, link_text)

                    pdf_links.append({
                        'title': enhanced_info['title'],
                        'url': pdf_url,
                        'version': language,
                        'original_filename': enhanced_info['filename'],
                        'enhanced_filename': enhanced_info['enhanced_filename']
                    })

                    self.logger.debug(f"âœ… æå–æ ¸å¿ƒPDF: {enhanced_info['title'][:50]}...")

        except Exception as e:
            self.logger.error(f"âŒ ä»pdfListæå–PDFå¤±è´¥: {str(e)}")

        return pdf_links

    def _extract_version_info(self, link_element) -> str:
        """ä»é“¾æ¥å…ƒç´ ä¸­æå–ç‰ˆæœ¬ä¿¡æ¯

        Args:
            link_element: é“¾æ¥çš„BeautifulSoupå…ƒç´ 

        Returns:
            str: ç‰ˆæœ¬ä¿¡æ¯ï¼Œå¦‚ "1.2026"
        """
        try:
            # æ–¹æ³•1ï¼šä»é“¾æ¥æ–‡æœ¬ä¸­æå–ç‰ˆæœ¬ä¿¡æ¯
            link_text = link_element.get_text(strip=True)

            # æŸ¥æ‰¾ç‰ˆæœ¬æ¨¡å¼ï¼š1.2026, Version 1.2026, 2026ç­‰
            # åŒ¹é…ç‰ˆæœ¬æ¨¡å¼ï¼šæ•°å­—.å¹´ä»½ æˆ– å¹´ä»½
            version_patterns = [
                r'(?:version\s+)?(\d{1,2})\.(\d{4})',  # Version 1.2026 æˆ– 1.2026
                r'(?:v\s*)(\d{1,2})\.(\d{4})',          # v1.2026
                r'(?:(\d{4}))',                        # 2026
                r'(?:(\d{1,2})\.(\d{2}))'              # 1.26
            ]

            for pattern in version_patterns:
                match = re.search(pattern, link_text, re.IGNORECASE)
                if match:
                    groups = match.groups()
                    if len(groups) == 2:
                        return f"{groups[0]}_{groups[1]}"
                    else:
                        return groups[0]

            # æ–¹æ³•2ï¼šæ£€æŸ¥URLä¸­çš„ç‰ˆæœ¬ä¿¡æ¯
            href = link_element.get('href', '')
            url_version_patterns = [
                r'(\d{4})',  # å¹´ä»½
                r'(\d{1,2})\.(\d{4})'  # æ•°å­—.å¹´ä»½
            ]

            for pattern in url_version_patterns:
                match = re.search(pattern, href)
                if match:
                    groups = match.groups()
                    if len(groups) == 2:
                        return f"{groups[0]}_{groups[1]}"
                    else:
                        return groups[0]

            # æ–¹æ³•3ï¼šå°è¯•ä»å‘¨å›´æ–‡æœ¬ä¸­æŸ¥æ‰¾
            parent = link_element.parent
            if parent:
                parent_text = parent.get_text(strip=True)
                for pattern in version_patterns:
                    match = re.search(pattern, parent_text, re.IGNORECASE)
                    if match:
                        groups = match.groups()
                        if len(groups) == 2:
                            return f"{groups[0]}_{groups[1]}"
                        else:
                            return groups[0]

            self.logger.debug(f"âš ï¸ æœªæ‰¾åˆ°ç‰ˆæœ¬ä¿¡æ¯: {link_text[:50]}")
            return "unknown"

        except Exception as e:
            self.logger.debug(f"âŒ æå–ç‰ˆæœ¬ä¿¡æ¯å¤±è´¥: {str(e)}")
            return "unknown"

    def _enhance_pdf_info(self, title: str, version_info: str, pdf_url: str = None) -> Dict[str, str]:
        """å¢å¼ºPDFä¿¡æ¯ï¼Œæ·»åŠ ç‰ˆæœ¬ä¿¡æ¯åˆ°æ ‡é¢˜å’Œæ–‡ä»¶å

        Args:
            title: åŸå§‹æ ‡é¢˜
            version_info: ç‰ˆæœ¬ä¿¡æ¯
            pdf_url: PDFçš„URLï¼ˆå¯é€‰ï¼Œç”¨äºæå–åŸå§‹æ–‡ä»¶åï¼‰

        Returns:
            Dict[str, str]: å¢å¼ºåçš„ä¿¡æ¯
        """
        try:
            # æ¸…ç†æ ‡é¢˜
            clean_title = title.strip()

            # ä¼˜å…ˆä»PDF URLä¸­æå–æ–‡ä»¶å
            filename_prefix = None
            if pdf_url:
                try:
                    from urllib.parse import urlparse
                    parsed_url = urlparse(pdf_url)
                    path = parsed_url.path

                    # æå–æ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰
                    import os
                    filename = os.path.basename(path)
                    if filename and '.' in filename:
                        filename_prefix = os.path.splitext(filename)[0]
                        # ç¡®ä¿æ–‡ä»¶åå®‰å…¨
                        filename_prefix = re.sub(r'[^\w\-_]', '_', filename_prefix)

                        self.logger.debug(f"ä»URLæå–æ–‡ä»¶å: {filename} -> {filename_prefix}")
                except Exception as e:
                    self.logger.debug(f"ä»URLæå–æ–‡ä»¶åå¤±è´¥: {str(e)}")

            # å¦‚æœæ— æ³•ä»URLæå–ï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘
            if not filename_prefix:
                # ç§»é™¤å¸¸è§å‰ç¼€
                prefixes_to_remove = [
                    'NCCN Guidelines',
                    'NCCN Guidelines for',
                    'NCCN',
                    'Guidelines',
                    'Guideline',
                    'Treatment Guidelines',
                    'Clinical Practice Guidelines'
                ]

                temp_prefix = clean_title
                for prefix in prefixes_to_remove:
                    if clean_title.lower().startswith(prefix.lower()):
                        temp_prefix = clean_title[len(prefix):].strip()
                        break

                # è¿›ä¸€æ­¥æ¸…ç†æ–‡ä»¶å
                filename_prefix = re.sub(r'\b(version|ver|v)\s*[\d\.]+\b', '', temp_prefix, flags=re.IGNORECASE)
                filename_prefix = re.sub(r'\b\d{4}\b', '', filename_prefix)  # ç§»é™¤å¹´ä»½
                filename_prefix = re.sub(r'\s+', '_', filename_prefix.strip())  # æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿
                filename_prefix = re.sub(r'[^\w\-_]', '_', filename_prefix)  # ç§»é™¤ç‰¹æ®Šå­—ç¬¦

            # ç¡®ä¿ä¸ä¸ºç©ºä¸”æœ‰æ„ä¹‰
            if not filename_prefix or len(filename_prefix) < 2:
                filename_prefix = "NCCN_Guideline"
            elif filename_prefix.lower() in ['guideline', 'guidelines', 'nccn', 'nccn_guideline']:
                filename_prefix = "NCCN_Guideline"

            # ç”Ÿæˆå¢å¼ºæ–‡ä»¶å
            if version_info != "unknown":
                enhanced_filename = f"{filename_prefix}_version_{version_info}.pdf"
                enhanced_title = f"{clean_title} (Version {version_info.replace('_', '.')})"
            else:
                enhanced_filename = f"{filename_prefix}.pdf"
                enhanced_title = clean_title

            return {
                'title': enhanced_title,
                'filename': filename_prefix,
                'enhanced_filename': enhanced_filename
            }

        except Exception as e:
            self.logger.error(f"âŒ å¢å¼ºPDFä¿¡æ¯å¤±è´¥: {str(e)}")
            return {
                'title': title,
                'filename': 'NCCN_Guideline',
                'enhanced_filename': f"NCCN_Guideline_{version_info}.pdf" if version_info != "unknown" else "NCCN_Guideline.pdf"
            }

    def _get_pdfs_from_detail_page_fallback(self, soup: BeautifulSoup, language_filter: str = 'all') -> List[Dict[str, Any]]:
        """ä¼ ç»Ÿæ–¹æ³•ï¼šä»è¯¦æƒ…é¡µé¢è·å–PDFï¼ˆå½“æ‰¾ä¸åˆ°Guidelineséƒ¨åˆ†æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            self.logger.info(f"ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æå–PDFï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰")

            pdf_links = []

            # æŸ¥æ‰¾æ‰€æœ‰PDFé“¾æ¥
            all_links = soup.find_all('a', href=True)

            for link in all_links:
                href = link.get('href', '')
                if href.endswith('.pdf'):
                    # åº”ç”¨è¯­è¨€è¿‡æ»¤
                    link_text = link.text.strip()
                    if not self._should_include_pdf(href, language_filter, link_text):
                        continue

                    # æ„å»ºå®Œæ•´çš„PDF URL
                    if href.startswith('http'):
                        pdf_url = href
                    else:
                        base_url = 'https://www.nccn.org'
                        if href.startswith('/'):
                            pdf_url = base_url + href
                        else:
                            pdf_url = urljoin(base_url, href)

                    title = link_text if link_text else "NCCN_Guideline"
                    # ç¡®å®šè¯­è¨€ç‰ˆæœ¬ - ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„è¯­è¨€å‚æ•°
                    if language_filter == 'chinese':
                        version = 'Chinese'
                    elif language_filter == 'english':
                        version = 'English'
                    else:
                        # é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹
                        version = self._detect_pdf_language(href, link_text)

                    # æå–ç‰ˆæœ¬ä¿¡æ¯
                    version_info = self._extract_version_info(link)
                    enhanced_info = self._enhance_pdf_info(title, version_info, pdf_url)

                    pdf_links.append({
                        'title': enhanced_info['title'],
                        'url': pdf_url,
                        'version': version,
                        'original_filename': enhanced_info['filename'],
                        'enhanced_filename': enhanced_info['enhanced_filename']
                    })

            self.logger.info(f"ğŸ“Š ä¼ ç»Ÿæ–¹æ³•æå–åˆ° {len(pdf_links)} ä¸ªPDF")
            return pdf_links

        except Exception as e:
            self.logger.error(f"âŒ ä¼ ç»Ÿæ–¹æ³•æå–PDFå¤±è´¥: {str(e)}")
            return []

    def _download_single_pdf(self, pdf_info: Dict[str, Any], theme_dir: Path) -> bool:
        """ä¸‹è½½å•ä¸ªPDFæ–‡ä»¶

        Args:
            pdf_info: PDFä¿¡æ¯å­—å…¸
            theme_dir: ç›®æ ‡ç›®å½•

        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        pdf_url = pdf_info['url']
        title = pdf_info['title']
        version = pdf_info.get('version', 'Unknown')

        # æ„å»ºæ–‡ä»¶å - ä¼˜å…ˆä½¿ç”¨å¢å¼ºçš„æ–‡ä»¶å
        if 'enhanced_filename' in pdf_info and pdf_info['enhanced_filename']:
            filename = pdf_info['enhanced_filename']
        else:
            # å›é€€åˆ°åŸå§‹æ–‡ä»¶åç”Ÿæˆé€»è¾‘
            safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
            filename = f"{safe_title}_{version}.pdf"

        # ç¡®ä¿æ–‡ä»¶åä»¥.pdfç»“å°¾
        if not filename.lower().endswith('.pdf'):
            filename = f"{filename}.pdf"

        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)

        filepath = theme_dir / filename

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰æ•ˆ
        if self._is_file_valid(filepath):
            self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œè·³è¿‡: {filename}")
            self.stats.skipped_files += 1
            return True

        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                self.logger.debug(f"ä¸‹è½½å°è¯• {attempt + 1}/{self.max_retries}: {title}")

                # é¢„å…ˆæ£€æŸ¥æ–‡ä»¶å¤§å°
                if not self._check_pdf_validity(pdf_url):
                    self.logger.warning(f"æ–‡ä»¶å¯èƒ½æ— æ•ˆï¼Œè·³è¿‡: {pdf_url}")
                    return False

                # è®¾ç½®ä¸‹è½½headers
                download_headers = {
                    'Accept': 'application/pdf,application/x-pdf,*/*',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Referer': 'https://www.nccn.org/',
                }

                # ä¸‹è½½æ–‡ä»¶
                response = self.session.get(
                    pdf_url,
                    headers=download_headers,
                    stream=True,
                    timeout=30
                )
                response.raise_for_status()

                # ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                temp_filepath = filepath.with_suffix('.tmp')

                # ä¸‹è½½å†…å®¹
                file_size = int(response.headers.get('content-length', 0))
                if file_size == 0:
                    file_size = None

                with open(temp_filepath, 'wb') as f, tqdm(
                    desc=filename[:50] + '...' if len(filename) > 50 else filename,
                    total=file_size,
                    unit='iB',
                    unit_scale=True,
                    unit_divisor=1024,
                    colour='green'
                ) as pbar:
                    downloaded_size = 0
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            size = f.write(chunk)
                            downloaded_size += size
                            pbar.update(size)

                # éªŒè¯PDFå†…å®¹
                if self._validate_pdf_content(temp_filepath):
                    # ç§»åŠ¨åˆ°æœ€ç»ˆä½ç½®
                    temp_filepath.rename(filepath)
                    self.stats.downloaded_size_mb += downloaded_size / (1024 * 1024)
                    self.logger.info(f"ä¸‹è½½å®Œæˆ: {filename} ({downloaded_size/1024/1024:.1f}MB)")
                    return True
                else:
                    temp_filepath.unlink(missing_ok=True)
                    self.logger.error(f"PDFéªŒè¯å¤±è´¥: {filename}")

            except Exception as e:
                self.logger.warning(f"ä¸‹è½½å°è¯• {attempt + 1} å¤±è´¥: {str(e)}")
                if attempt < self.max_retries - 1:
                    wait_time = self.retry_delay * (attempt + 1)
                    self.logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    self.logger.error(f"ä¸‹è½½å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {title}")
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    temp_filepath = filepath.with_suffix('.tmp')
                    temp_filepath.unlink(missing_ok=True)

        return False

    def _is_file_valid(self, filepath: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ

        Args:
            filepath: æ–‡ä»¶è·¯å¾„

        Returns:
            bool: æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        """
        if not filepath.exists():
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = filepath.stat().st_size
        if file_size < self.min_file_size:
            return False

        # æ£€æŸ¥PDFæ–‡ä»¶å¤´
        try:
            with open(filepath, 'rb') as f:
                header = f.read(4)
                return header == b'%PDF'
        except:
            return False

    def _check_pdf_validity(self, pdf_url: str) -> bool:
        """æ£€æŸ¥PDFæ–‡ä»¶æœ‰æ•ˆæ€§

        Args:
            pdf_url: PDFæ–‡ä»¶URL

        Returns:
            bool: PDFæ˜¯å¦æœ‰æ•ˆ
        """
        try:
            response = self.session.head(pdf_url, timeout=10)
            response.raise_for_status()

            content_length = response.headers.get('content-length', 0)
            if content_length:
                size = int(content_length)
                return size >= self.min_file_size

            return True

        except:
            return True  # HEADè¯·æ±‚å¤±è´¥æ—¶ï¼Œå‡è®¾æ–‡ä»¶æœ‰æ•ˆï¼Œç»§ç»­å°è¯•ä¸‹è½½

    def _validate_pdf_content(self, filepath: Path) -> bool:
        """éªŒè¯PDFæ–‡ä»¶å†…å®¹

        Args:
            filepath: æ–‡ä»¶è·¯å¾„

        Returns:
            bool: PDFå†…å®¹æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            with open(filepath, 'rb') as f:
                # æ£€æŸ¥PDFæ–‡ä»¶å¤´
                header = f.read(4)
                if header != b'%PDF':
                    return False

                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                f.seek(0, 2)  # ç§»åˆ°æ–‡ä»¶æœ«å°¾
                if f.tell() < self.min_file_size:
                    return False

                return True
        except:
            return False

    def _generate_download_report(self, theme: ThemeConfig):
        """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š

        Args:
            theme: ä¸»é¢˜é…ç½®
        """
        report = {
            'theme': theme.display_name,
            'timestamp': datetime.now().isoformat(),
            'stats': self.stats.to_dict()
        }

        # ä¿å­˜JSONæŠ¥å‘Š
        report_date = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.logs_dir / f'stats_{theme.name}_{report_date}.json'

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self.logger.info("=== ä¸‹è½½ç»Ÿè®¡æŠ¥å‘Š ===")
        self.logger.info(f"ä¸»é¢˜: {theme.display_name}")
        self.logger.info(f"æ€»æ–‡ä»¶æ•°: {self.stats.total_files}")
        self.logger.info(f"æˆåŠŸä¸‹è½½: {self.stats.successful_files}")
        self.logger.info(f"è·³è¿‡(å·²å­˜åœ¨): {self.stats.skipped_files}")
        self.logger.info(f"ä¸‹è½½å¤±è´¥: {self.stats.failed_files}")
        self.logger.info(f"æˆåŠŸç‡: {self.stats.success_rate:.1f}%")
        self.logger.info(f"æ€»è€—æ—¶: {self.stats.duration_seconds:.1f}ç§’")
        self.logger.info(f"å¹³å‡é€Ÿåº¦: {self.stats.avg_speed_mbps:.2f}MB/s")
        self.logger.info(f"ä¸‹è½½æ•°æ®é‡: {self.stats.downloaded_size_mb:.1f}MB")
        self.logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    def _handle_failed_downloads(self, failed_files: List[str], theme_dir: Path):
        """å¤„ç†å¤±è´¥ä¸‹è½½çš„æ–‡ä»¶

        Args:
            failed_files: å¤±è´¥æ–‡ä»¶åˆ—è¡¨
            theme_dir: ä¸»é¢˜ç›®å½•
        """
        if not failed_files:
            return

        print(f"\næœ‰ {len(failed_files)} ä¸ªæ–‡ä»¶ä¸‹è½½å¤±è´¥:")
        for i, filename in enumerate(failed_files, 1):
            print(f"  {i}. {filename}")

        # è¯¢é—®æ˜¯å¦é‡æ–°ä¸‹è½½
        print(f"\næ˜¯å¦é‡æ–°å°è¯•ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶? (y/n): ", end='')
        try:
            choice = input().lower().strip()
            if choice in ['y', 'yes', 'æ˜¯']:
                self._retry_failed_downloads(failed_files, theme_dir)
        except KeyboardInterrupt:
            print("\nç”¨æˆ·å–æ¶ˆæ“ä½œ")

    def _retry_failed_downloads(self, failed_files: List[str], theme_dir: Path):
        """é‡æ–°ä¸‹è½½å¤±è´¥çš„æ–‡ä»¶

        Args:
            failed_files: å¤±è´¥æ–‡ä»¶åˆ—è¡¨
            theme_dir: ä¸»é¢˜ç›®å½•
        """
        self.logger.info(f"å¼€å§‹é‡æ–°ä¸‹è½½ {len(failed_files)} ä¸ªå¤±è´¥æ–‡ä»¶...")

        # é‡æ–°è·å–PDFé“¾æ¥å¹¶åŒ¹é…å¤±è´¥çš„æ–‡ä»¶
        for theme in self.THEMES.values():
            if theme.directory == theme_dir.name:
                pdf_links = self._get_pdf_links(theme)

                for failed_file in failed_files:
                    # æŸ¥æ‰¾åŒ¹é…çš„PDFé“¾æ¥
                    for pdf_info in pdf_links:
                        if failed_file in pdf_info['title']:
                            self.logger.info(f"é‡æ–°ä¸‹è½½: {failed_file}")
                            success = self._download_single_pdf(pdf_info, theme_dir)

                            if success:
                                self.logger.info(f"é‡æ–°ä¸‹è½½æˆåŠŸ: {failed_file}")
                            else:
                                self.logger.error(f"é‡æ–°ä¸‹è½½å¤±è´¥: {failed_file}")

                            break
                break


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("    NCCNæŒ‡å—ä¸‹è½½å·¥å…· v2.0")
    print("    ä¼˜åŒ–çš„èœå•å¼ä¸‹è½½å·¥å…·")
    print("    ä½œè€…: Claude Code")
    print("=" * 60)

    # è¯»å–é…ç½®æ–‡ä»¶
    config_file = 'config.json'
    if not os.path.exists(config_file):
        print(f"âŒ é…ç½®æ–‡ä»¶ {config_file} ä¸å­˜åœ¨")
        print("è¯·åˆ›å»ºconfig.jsoné…ç½®æ–‡ä»¶")
        return

    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        print(f"âœ… æˆåŠŸè¯»å–é…ç½®æ–‡ä»¶ {config_file}")
    except Exception as e:
        print(f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return

    config = {}

    try:
        # ä»é…ç½®è·å–è®¤è¯æ–¹å¼
        auth_config = config_data.get('authentication', {})
        method = auth_config.get('method', 'username_password')
        username = auth_config.get('username', '')
        password = auth_config.get('password', '')
        cookie_file = auth_config.get('cookie_file', 'extracted_cookies.txt')

        print(f"\nğŸ“‹ ä½¿ç”¨è®¤è¯æ–¹å¼: {method}")

        if method == 'username_password':
            if not username or not password or password == 'your_password_here':
                print("âŒ ç”¨æˆ·å/å¯†ç è®¤è¯é…ç½®ä¸å®Œæ•´")
                print(f"   è¯·åœ¨config.jsonä¸­è®¾ç½®æ­£ç¡®çš„ç”¨æˆ·åå’Œå¯†ç ")
                print(f"   å½“å‰ç”¨æˆ·å: {username if username else 'æœªè®¾ç½®'}")
                print(f"   å½“å‰å¯†ç : {'å·²è®¾ç½®' if password and password != 'your_password_here' else 'æœªè®¾ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼'}")
                return

            config['auth_method'] = 'username_password'
            config['username'] = username
            config['password'] = password
            print(f"âœ… ä½¿ç”¨ç”¨æˆ·åè®¤è¯: {username}")

        elif method == 'cookie':
            if not os.path.exists(cookie_file):
                print(f"âŒ Cookieæ–‡ä»¶ {cookie_file} ä¸å­˜åœ¨")
                print(f"è¯·ç¡®ä¿extracted_cookies.txtæ–‡ä»¶å­˜åœ¨")
                return

            try:
                with open(cookie_file, 'r', encoding='utf-8') as f:
                    cookie_content = f.read().strip()
                if not cookie_content:
                    print(f"âŒ Cookieæ–‡ä»¶ {cookie_file} ä¸ºç©º")
                    return

                config['auth_method'] = 'cookie'
                config['cookie_file'] = cookie_file
                print(f"âœ… ä½¿ç”¨Cookieè®¤è¯: {cookie_file}")
            except Exception as e:
                print(f"âŒ è¯»å–Cookieæ–‡ä»¶å¤±è´¥: {e}")
                return
        else:
            print(f"âŒ ä¸æ”¯æŒçš„è®¤è¯æ–¹å¼: {method}")
            return

        # åˆå§‹åŒ–ä¸‹è½½å™¨
        downloader = NCCNDownloaderV2(config)

        # æµ‹è¯•è®¤è¯
        print("\næ­£åœ¨æµ‹è¯•è®¤è¯...")
        if not downloader.authenticate():
            print("è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥è®¤è¯ä¿¡æ¯")
            return

        print("è®¤è¯æˆåŠŸ!\n")

        # ä¸»èœå•å¾ªç¯
        while True:
            print("\n" + "=" * 60)
            print("ä¸»èœå• - è¯·é€‰æ‹©è¦ä¸‹è½½çš„ä¸»é¢˜:")
            print("=" * 60)

            for key, theme in NCCNDownloaderV2.THEMES.items():
                print(f"{key}. {theme.display_name}")
                print(f"   {theme.description}")
                print(f"   ç›®å½•: {theme.directory}")
                print()

            print("7. æŸ¥çœ‹ä¸‹è½½ç»Ÿè®¡")
            print("8. é€€å‡º")

            try:
                choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-8): ").strip()

                if choice == '8':
                    print("æ„Ÿè°¢ä½¿ç”¨NCCNä¸‹è½½å·¥å…·!")
                    break
                elif choice == '7':
                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    print("\nè¯·å…ˆè¿è¡Œä¸‹è½½ä»¥è·å–ç»Ÿè®¡ä¿¡æ¯")
                    input("æŒ‰å›è½¦é”®ç»§ç»­...")
                elif choice in NCCNDownloaderV2.THEMES:
                    # ä¸‹è½½æŒ‡å®šä¸»é¢˜
                    theme = NCCNDownloaderV2.THEMES[choice]

                    # è¯­è¨€è¿‡æ»¤é€‰é¡¹
                    language_filter = 'all'  # é»˜è®¤å…¨éƒ¨
                    if theme.has_language_filter:
                        if theme.category == 'category_1':
                            # ç™Œç—‡æ²»ç–—æŒ‡å—ï¼šé»˜è®¤åªä¸‹è½½è‹±æ–‡ç‰ˆæœ¬
                            language_filter = 'english'
                            print(f"\nğŸ“‹ ç™Œç—‡æ²»ç–—æŒ‡å—å°†åªä¸‹è½½è‹±æ–‡ç‰ˆæœ¬")
                        elif theme.category == 'supportive_care':
                            print(f"\nğŸ“‹ è¯­è¨€è¿‡æ»¤é€‰é¡¹ (é€‚ç”¨äºæ”¯æŒæ€§æŠ¤ç†æŒ‡å—):")
                            print("1. å…¨éƒ¨ç‰ˆæœ¬ (è‹±æ–‡ + ä¸­æ–‡)")
                            print("2. ä»…è‹±æ–‡ç‰ˆæœ¬")

                            while True:
                                lang_choice = input("\nè¯·é€‰æ‹©è¯­è¨€è¿‡æ»¤ (1-2, é»˜è®¤1): ").strip()
                                if not lang_choice:
                                    lang_choice = '1'

                                if lang_choice == '1':
                                    language_filter = 'all'
                                    break
                                elif lang_choice == '2':
                                    language_filter = 'english'
                                    break
                                else:
                                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")
                        elif theme.category == 'patient_guidelines_bilingual':
                            print(f"\nğŸ“‹ è¯­è¨€è¿‡æ»¤é€‰é¡¹ (é€‚ç”¨äºåŒè¯­æ‚£è€…æŒ‡å—):")
                            print("1. å…¨éƒ¨ç‰ˆæœ¬ (è‹±æ–‡ + ä¸­æ–‡)")
                            print("2. ä»…è‹±æ–‡ç‰ˆæœ¬")
                            print("3. ä»…ä¸­æ–‡ç‰ˆæœ¬")

                            while True:
                                lang_choice = input("\nè¯·é€‰æ‹©è¯­è¨€è¿‡æ»¤ (1-3, é»˜è®¤1): ").strip()
                                if not lang_choice:
                                    lang_choice = '1'

                                if lang_choice == '1':
                                    language_filter = 'all'
                                    break
                                elif lang_choice == '2':
                                    language_filter = 'english'
                                    break
                                elif lang_choice == '3':
                                    language_filter = 'chinese'
                                    break
                                else:
                                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-3 ä¹‹é—´çš„æ•°å­—")
                        else:
                            # å…¶ä»–éœ€è¦è¯­è¨€è¿‡æ»¤çš„æƒ…å†µ
                            print(f"\nğŸ“‹ è¯­è¨€è¿‡æ»¤é€‰é¡¹:")
                            print("1. å…¨éƒ¨ç‰ˆæœ¬ (è‹±æ–‡ + ä¸­æ–‡)")
                            print("2. ä»…è‹±æ–‡ç‰ˆæœ¬")
                            print("3. ä»…ä¸­æ–‡ç‰ˆæœ¬")

                            while True:
                                lang_choice = input("\nè¯·é€‰æ‹©è¯­è¨€è¿‡æ»¤ (1-3, é»˜è®¤1): ").strip()
                                if not lang_choice:
                                    lang_choice = '1'

                                if lang_choice == '1':
                                    language_filter = 'all'
                                    break
                                elif lang_choice == '2':
                                    language_filter = 'english'
                                    break
                                elif lang_choice == '3':
                                    language_filter = 'chinese'
                                    break
                                else:
                                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-3 ä¹‹é—´çš„æ•°å­—")

                    print(f"\nå¼€å§‹ä¸‹è½½: {theme.display_name}")
                    print(f"ç›®å½•: {theme.directory}")
                    if theme.has_language_filter:
                        filter_desc = {
                            'all': 'å…¨éƒ¨ç‰ˆæœ¬ (è‹±æ–‡ + ä¸­æ–‡)',
                            'english': 'ä»…è‹±æ–‡ç‰ˆæœ¬',
                            'chinese': 'ä»…ä¸­æ–‡ç‰ˆæœ¬'
                        }
                        print(f"è¯­è¨€è¿‡æ»¤: {filter_desc[language_filter]}")
                    print("-" * 40)

                    success = downloader.download_theme(choice, language_filter)

                    if success:
                        print(f"\nâœ… {theme.display_name} ä¸‹è½½å®Œæˆ!")
                    else:
                        print(f"\nâŒ {theme.display_name} ä¸‹è½½å¤±è´¥!")

                    input("\næŒ‰å›è½¦é”®è¿”å›ä¸»èœå•...")
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-7 ä¹‹é—´çš„æ•°å­—")

            except KeyboardInterrupt:
                print("\n\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
                break
            except Exception as e:
                print(f"\nå‘ç”Ÿé”™è¯¯: {str(e)}")
                input("æŒ‰å›è½¦é”®ç»§ç»­...")

    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()